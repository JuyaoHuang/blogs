---
title: "GPT-Sovits训练参数介绍"
published: 2025-10-11
tags: ['gpt-sovits']
description: '介绍gpt-sovits模型训练和推理参数'
first_level_category: "项目实践"
second_level_category: "GPT-Sovits"
author: "Alen"
draft: false
---


## 1Bb-SoVITS Training 参数详解



SoVITS模型主要负责学习和复刻**音色（timbre）、韵律（prosody）和音高（pitch）**。

这是决定最终合成声音“像不像”的关键。

### Batch size per GPU

- **意义**:     一次性“喂”给显卡多少条音频数据进行处理。
- **目的**:     在显存允许的范围内，尽可能增大batch_size可以加快训练速度，并可能使训练过程更稳定。
- **作用与选择**:
  - **值越大**: 训练速度越快，对显存的占用也越大。
  - **值越小**: 训练速度变慢，但能有效避免“显存溢出”（CUDA out of memory）的错误。
  - **配置建议**:     **一般从显卡显存的一半开始尝试，如果溢出(CUDA out of memory)，再往下调到 6或者4**

### Total epochs

- **意义**:     整个数据集被完整地训练多少遍。

- **目的**:     决定模型训练的充分程度。

- **作用与选择**:
  - **轮数太少**: 模型没学好，音色不像，可能会有很多杂音，称为“欠拟合”（Underfitting）。
  
  - **轮数太多**: 模型可能“学过头”了，把数据集里的瑕疵、口癖甚至噪音都学了进去，导致泛化能力下降，对新文本的合成效果变差，称为“过拟合”（Overfitting）。
  
  - **配置建议**:
    
    - 文档推荐 30 左右。    对于高质量的数据集（如我的ATRI数据集），这是一个非常合理的范围。
    
    - 建议首次训练时设置为 20 或 25。
    
      ​	这样可以在较短时间内得到一个效果不错的模型，方便评估。如果效果已经很好，就不必继续增加；如果还有提升空间，下次再增加到 30。
    
    - 16 也可以，但可能稍显不足。

### Save only the latest weight file to save disk space 

- **意义**: 是否在每次保存时，用新的模型文件覆盖掉旧的模型文件。
- **目的**: 节省服务器的硬盘空间。
- **作用与选择**:
  - **勾选**: SoVITS_weights 文件夹里将始终只有一个最新的 sovits_xxx.pth 文件。这对于硬盘空间有限的服务器非常友好。
  - **不勾选**: 每隔一定的epoch，就会保存一个模型文件（如sovits_4.pth, sovits_8.pth...）。这会占用大量空间，但好处是可以回头去比较不同训练阶段的模型效果，找到效果最好的那个版本。

### Save a small final model to the 'weights' folder at each save point 

- **意义**: 在每次保存完整模型的同时，额外保存一个体积小、只能用于推理（不能用于继续训练）的模型。
- **目的**: 方便快速评估不同训练阶段的模型效果，而无需加载巨大的完整模型。
- **作用与选择**:
  - **勾选**: 每次保存时，会在 SoVITS_weights/实验名/ 文件夹下生成一个以 _e (epoch) 结尾的小模型文件。可以在推理界面随时加载这些小模型试听，判断训练进行到哪个epoch时效果最好。
  - **不勾选**: 将无法方便地对比不同阶段的模型。
  - **配置建议**: 勾选。这是监控训练进程、防止过拟合的关键功能。

### Text model learning rate weighting

- **意义**: SoVITS模型中有一个与文本编码器相关的部分。这个参数用于调整该部分学习的“快慢”。
- **目的**: 平衡音色学习和内容学习。
- **作用与选择**:
  - 这是一个比较高级的参数，通常**保持默认值 0.4** 即可。
  - 如果发现模型有吐字不清等问题，可以尝试微调此参数，但对于初次训练，不建议修改。

### Save frequency (save_every_epoch)

- **意义**: 每隔多少个 epoch 保存一次模型。
- **目的**: 平衡保存频率和训练中断的风险。
- **作用与选择**:
  - **值越小 (如 1)**: 保存得越频繁。好处是如果训练意外中断，损失的进度较少。坏处是频繁的磁盘I/O操作可能会稍微拖慢训练速度。
  - **值越大 (如 5)**: 保存间隔长。
  - **配置建议**: 4 是一个非常平衡的选择。

### GPU number is separated by each GPU will run one process

- **意义**: 指定使用哪一块（或哪几块）GPU进行训练。
- **目的**: 在多GPU服务器上分配任务。
- **作用与选择**:
  - 服务器通常只有一块GPU，其编号为 0。
  - **配置建议**: 保持默认的 0。

------



## 1Bb-GPT Training 参数详解



GPT模型在整个流程中扮演着 **“大脑”** 的角色。它不负责具体发声，而是学习**文本内容与语音韵律（停顿、重音、情感等）之间的映射关系**。

一个好的GPT模型，能让最终合成的语音听起来不像“机器人朗读”，而是充满自然感和情感表现力。

### Batch size per GPU

- **意义**:     和SoVITS一样，一次性处理多少条文本-音频数据对。
- **目的**:     平衡训练速度和显存占用。
- **作用与选择**:
  - GPT模型的训练通常比SoVITS**更吃显存**。
  - 如果SoVITS能跑到batch_size=8，GPT可能只能跑到 6 或 4。
  - **配置建议**:   和Sovits一样，从单卡显存的一半开始尝试，如果遇到显存溢出错误，果断**降低到 6 或 4**。这是GPT训练中最常见的调整项。

### Total training epochs (total_epoch)

- **意义**:     整个数据集被完整地训练多少遍。
- **目的**:     让GPT模型充分学习文本和韵律的对应关系。
- **作用与选择**:
  - GPT的收敛（学好）通常比SoVITS要快一些。
  - 文档和社区经验普遍认为，GPT的训练轮数不需要像SoVITS那么多。
  - **配置建议**:
    - **20**。它既能保证模型学得充分，又不容易过拟合。
    - 对于高质量、时长较长（如30分钟以上）的数据集，15 到 20 轮通常就能达到非常好的效果。

### Save only the latest weight file to save disk space

- **意义**:     是否用新的GPT模型文件覆盖旧的。
- **目的**:     节省硬盘空间。
- **作用与选择**:
  - **勾选**:    GPT_weights 文件夹里将始终只有一个最新的 gpt_xxx.pth 文件。GPT模型文件同样很大，勾选此项可以节省大量空间。
  - **不勾选**:     会保存每个保存点的GPT模型，占用巨大空间。

### Save a small final model to the 'weights' folder at each save point

- **意义**:     在保存完整GPT模型的同时，额外保存一个用于推理的小模型。
- **目的**:     方便快速评估和对比不同训练阶段的GPT模型效果。
- **作用与选择**:
  - **勾选**:     每次保存时，会在 GPT_weights/实验名/ 文件夹下生成一个以 _e (epoch) 结尾的GPT小模型。这是判断模型韵律和自然度是否提升的**关键**。
  - **不勾选**:     将失去监控GPT训练进度的重要手段。
  - **配置建议**:      **勾选**。

### Save frequency (save_every_epoch) 

- **意义**:     每隔多少个 epoch 保存一次GPT模型。
- **目的**:     平衡保存频率和训练中断的风险。
- **作用与选择**:
  - GPT的训练通常比较稳定，可以将保存间隔设置得比SoVITS稍长一些。
  - **配置建议**:     5  是一个非常好的选择。如果总轮数是20，那么将在第5、10、15、20轮得到4个可供评估的小模型，这完全足够。

### Enable DPO Training (Experimental) 

- **意义**:     DPO (Direct Preference Optimization) 是一种更先进的对齐技术，旨在让模型的输出更符合人类的偏好。
- **目的**:     （理论上）可以提升模型的自然度和表现力。
- **作用与选择**:
  - 这是一个**实验性功能**，可能不稳定，且需要额外的数据集和配置。
  - 对于标准的语音复刻任务，**完全不需要**开启此项。

### GPU number is separated by -, each GPU will run one process

- **意义**: 指定使用哪一块GPU进行训练。
- **目的**: 分配硬件资源。
- **作用与选择**:
  - **配置建议**:     和SoVITS一样，**保持默认的 0**。

------

## 推理界面参数详解

### Model switch

- **GPT weight list**:     GPT模型列表。
- **SoVITS weight list**:     SoVITS模型列表。
- **意义**:     选择要用来合成语音的GPT和SoVITS模型。
- **目的**:     加载训练好的模型权重。
- **操作**:
  1. 点击橙色的 **refreshing model paths** 按钮，这会重新扫描 GPT_weights 和 SoVITS_weights 文件夹，将所有可用的小模型（以 _e 结尾的）加载到下拉列表中。
  2. 从下拉列表中，选择认为效果最好的那个**epoch**的模型组合。例如，...-e20.ckpt 和 ...e20_s1100.pth，这代表在第20轮训练时保存的模型。
  3. **技巧**:     可以多选几个不同epoch的模型来试听，比如第15轮和第20轮的，看看哪个模型的音色和韵律更符合预期。

### Please upload and fill reference information

这一部分是整个推理过程的**灵魂**，它决定了合成语音的**音色、情感和韵律**。

- **左侧的音频上传/录制区**:
  - **意义**:     提供一段“参考音频”（也叫“Prompt”）。模型会分析这段音频的**音色和说话风格**，然后用这种风格去说下面输入的文本。
  - **目的**:     控制合成语音的风格。
  - **操作**:
    - **上传**:     点击上传按钮，从**原始的语音数据集**中，选择一段**纯净、无背景音、情感鲜明**的3到10秒短音频。
    - **录制**:     你也可以自己录一段话，模型会模仿音色和语调。
  - **关键点**:     **参考音频的质量直接决定了合成语音的质量**
- **Enable no reference mode**:
  - **意义**: 完全不使用参考音频，让模型自己决定用什么音色和韵律。
  - **目的**: 在没有合适的参考音频时使用。
  - **操作**:
    - **不勾选**: 必须提供参考音频。
    - **勾选**: 模型会使用一个“平均”或“默认”的音色来合成。通常效果不如有参考的好。
- **Text for reference audio (参考音频的文本)**:
  - **意义**:     告诉模型，上传的那段参考音频里说的是什么内容。
  - **目的**:     帮助模型更精确地将“声音”和“内容”解耦，从而更纯粹地学习到音色和韵律。
  - **操作**:
    - **填写 (推荐)**:     尽可能准确地将参考音频中的台词手打进去。
    - **留空**:     如果留空，模型会进入“半参考模式”，效果会稍差一些。
  - **结论**: **上传参考音频 + 填写对应文本 = 最佳效果**。
- **Language for reference audio (参考音频的语言)**:
  - **意义**: 明确告知模型参考音频的语言。
  - **目的**: 激活正确的文本处理模块。

------

### Please fill in the target text and language mode for synthesis

这一部分是你想要让模型“说出来”的内容。

- **Inference text (待合成文本)**:
  - **意义**: 希望模型合成的文字内容。
  - **操作**: 在这里输入任意你想让模型说的日文句子。例如，输入 こんにちは、世界！
- **Inference text language (待合成文本的语言)**:
  - **意义**: 输入的文本是什么语言。
  - **目的**: 调用正确的语言模型。
  - **操作**: **选择 Japanese (日语)**。
- **How to slice the sentence (如何切分句子)**:
  - **意义**: 对于长文本，模型会按什么规则自动切成小段来合成，然后再拼接起来。
  - **目的**: 防止长文本合成时显存溢出或效果下降。
  - **操作**:
    - **默认选项通常是最好的** (按标点符号切分 或 Slice once every 4 sentences等)。
    - 如果输入的文本很短（一两句话），这个选项影响不大。
    - **建议**:     **保持默认即可**。
- **Adjust speech rate... 和 Adjust the speech rate and tone...**:
  - **意义**: 微调合成语音的语速、音调。
  - **目的**: 进行一些个性化调整。
  - **操作**: 首次合成时**不要勾选**。在得到一个基础版语音后，如果觉得语速太快或太慢，再勾选并拖动滑块进行微调。

------

### 完整操作流程总结

1. **刷新并选择模型**:     点击 refreshing model paths，从下拉列表中选择训练好的GPT和SoVITS模型（比如第20轮的）。
2. **准备参考物料**:
   - 点击上传按钮，从数据集中找一段3-10秒、清晰、有代表性的**日语**音频。
   - 在 Text for reference audio 框中，**打出这段音频的日文台词**。
   - 在 Language for reference audio 下拉菜单中，选择 **Japanese**。
3. **输入目标内容**:
   - 在 Inference text 框中，输入想让ATRI说的**日文句子**。
   - 在 Inference text language 下拉菜单中，选择 **Japanese**。
4. **合成**:
   - 页面上会有一个“**合成语音**”或类似的按钮，点击它。
5. **试听与迭代**:
   - 等待几秒钟，右侧会生成一个可播放的音频，这就是结果
   - 如果不满意，可以尝试更换不同的**参考音频**，或者在下拉菜单中选择**不同训练轮数**的模型，然后再次合成，直到找到最满意的组合。

### 高级推理参数详解

这部分参数主要分为两类：**语速音调调整** 和 **GPT采样参数**。

#### Adjust speech rate, higher for faster (调整语速，越高越快)

- **Adjust the speech rate and tone... (勾选框)**:
  - **意义**:     启用对上次合成结果的语速和音调进行微调，以防止结果过于随机。
  - **目的**:     这是一个高级功能，用于在连续合成多句话时，保持风格的一致性。
  - **建议**:     **通常保持不勾选**。除非在做长文本对话生成，否则用处不大。
- **Speech rate (语速)**:
  - **意义**:     控制合成语音的快慢。1 代表正常语速。
  - **目的**:     让声音更快或更慢。
  - **作用与选择**:
    - \> 1 (例如 1.2): 语速变快。
    - < 1 (例如 0.8): 语速变慢。
    - **建议**: 先用默认值 1 生成一次。如果觉得模型说话太慢或太快，再回来调整这个值。通常在 0.8 到 1.3 的范围内效果比较自然。
- **Pause Duration between Sentences (Seconds) (句间停顿秒数)**:
  - **意义**:     输入多句话时，句子之间的停顿时间。
  - **目的**:     控制对话的节奏感。
  - **作用与选择**:
    - 0.3 (默认) 是一个比较自然的短暂停顿。
    - 如果希望句子连接更紧凑，可以调小（如 0.1）。
    - 如果希望营造思考或喘气的效果，可以调大（如 0.5 或 0.8）。
    - **建议**:     **保持默认的 0.3 即可**。

------

#### GPT sampling parameters

这部分是整个推理过程的  **“创意控制器”**。GPT模型在生成韵律时，并不是只有一个唯一的“正确答案”，它会根据这些参数在一个概率分布中进行“采样”，决定最终的说话方式。

**核心思想：这些参数控制着模型的“随机性”或“创造性”。**

- **top_k**:
  - **意义**:     在所有可能的下一个词（或音素）中，只考虑概率最高的 k 个作为候选。
  - **目的**:     限制模型的选择范围，防止它选到一些非常不靠谱、概率极低的词，从而产生怪异的韵律或发音。
  - **作用与选择**:
    - **值越小 (如 5)**:     模型越“循规蹈矩”，生成的韵律更稳定、可预测，但可能略显呆板。
    - **值越大 (如 20)**:     模型选择范围更广，更有可能产生意想不到的、富有变化的韵律，但也增加了“翻车”的风险。
    - **建议**: **保持默认的 15 或 20 是一个很好的平衡点**。对于已经微调好的模型，这个值比较安全。如果在**无参考模式**下运行，可以适当调低到5来增加稳定性。
- **top_p**:
  - **意义**:     另一种限制选择范围的方法。它会从概率最高的词开始，不断累加它们的概率，直到总和达到 p 为止，然后从这些词中进行采样。
  - **目的**:     动态地控制选择范围的大小，比top_k更智能。
  - **作用与选择**:
    - top_p 和 top_k 通常**只使用其中一个**（程序会自动判断，如果top_p < 1，则优先使用top_p）。
    - **值越小 (如 0.6)**:     非常保守，只在最有把握的几个选项里选，结果稳定。
    - **值越大 (如 1)**:     相当于禁用了top_p，让模型可以考虑所有可能性。
    - **建议**: **保持默认值 1**，这样主要由top_k来控制。如果想尝试top_p，可以把top_k设置得很大（如100），然后调整top_p到 0.8 左右。
- **temperature (温度)**:
  - **意义**:     控制概率分布的“平滑度”，直接影响模型的“想象力”。
  - **目的**:     调整生成结果的随机性和多样性。
  - **作用与选择**:
    - **值越小 (如 0.6)**:     概率分布更“尖锐”，模型更倾向于选择概率最高的那个选项。结果非常稳定，重复性高，但缺乏变化。
    - **值越大 (如 1.0 或 1.2)**:     概率分布更“平缓”，所有选项的概率差距变小，模型更有可能选择一些次优但有创意的选项。结果更多样、更富表现力，但随机性也更高，可能会产生一些不自然的语调。
    - **建议**:   **1.0 是一个非常中性且安全的默认值**。如果你希望模型的语气更活泼、更多变，可以尝试提高到 1.1。如果希望她的语气非常平稳、没有意外，可以降低到 0.7 或 0.8。

## 打包模型

### 第一步：在服务器上找到并打包模型文件

GPT-SoVITS模型通常包含多个文件，并且存放在之前设置的“实验名称”对应的文件夹下。

#### 1. 找到你的模型目录

模型文件通常保存在 GPT_SoVITS 目录下的 GPT_weights 和 SoVITS_weights 文件夹中，并按照实验名称进行组织。

假设你的**实验名称**是 ATRI-v1 (你在训练时设置的)。
那么模型文件应该在：

- **GPT模型**：/root/GPT_SoVITS/GPT_weights/ATRI-v1/
- **SoVITS模型**：/root/GPT_SoVITS/SoVITS_weights/ATRI-v1/

在SSH终端中，使用 ls 命令确认这些目录是否存在，以及里面包含了训练出的模型文件（通常是 .ckpt 和 .pth 结尾的小模型文件，以及可能的完整模型文件）。

```bash
ls /root/GPT_SoVITS/GPT_weights/ATRI-v1/
ls /root/GPT_SoVITS/SoVITS_weights/ATRI-v1/
```

#### 2. 将模型文件打包成一个压缩包

为了方便下载，我们将这些模型文件打包到一个 .zip 压缩包中。最好把GPT和SoVITS这两个文件夹各自打包，或者打包它们的共同父目录。

**推荐做法：打包整个实验结果文件夹**

通常，模型训练结果会有一个公共的根目录，比如 logs/实验名/。或者更直接一点，就是把 GPT_weights/ATRI-v1/ 和 SoVITS_weights/ATRI-v1/ 这两个文件夹一起打包。

假设我们把它们打包到 ~/your_atri_model.zip。

```bash
# 确保你在 ~ (root) 目录下，或者到 /root/GPT_SoVITS/
cd /root/GPT_SoVITS/

# 安装 zip 工具 (如果系统没有的话)
sudo apt install -y zip

# 打包命令：将 GPT_weights/ATRI-v1 和 SoVITS_weights/ATRI-v1 两个文件夹打包
# 将 ATRI-v1 替换为你实际的实验名称
zip -r ~/your_atri_model.zip GPT_weights/ATRI-v1 SoVITS_weights/ATRI-v1
```

- zip -r:     zip 命令，-r 表示递归地打包文件夹及其内容。
- ~/your_atri_model.zip:     生成的压缩包的路径和名称。~ 代表 /root/。
- GPT_weights/ATRI-v1 SoVITS_weights/ATRI-v1:     要打包的两个文件夹的路径（相对于当前所在的 /root/GPT_SoVITS/ 目录）。

执行完毕后，你会在 /root/ 目录下找到 your_atri_model.zip 文件。

------

### 第二步：从服务器将打包好的文件下载到本地

这一步需要回到**本地电脑**，使用 scp 命令或者其他图形化工具。

#### 使用 scp 命令下载

scp 是最常用的命令行文件传输工具。

1. **在本地电脑的终端中**，运行以下命令：

   ```bash
   # 语法: scp <用户名>@<服务器IP>:<服务器文件路径> <本地目标路径>
   scp root@119.3.250.164:/root/your_atri_model.zip /path/to/your/local/deskto/
   ```

   - root@119.3.250.164:     服务器登录信息。

   - /root/your_atri_model.zip:     服务器上压缩包的完整路径。

   - /path/to/your/local/desktop/:     本地电脑上希望保存文件的目录（例如 C:\Users\YourName\Desktop\ 或者 /Users/YourName/Desktop/）。

   - **注意**：IPv6使用 双引号`""` 包括服务器/本地 文件路径

     ```bash
     scp "root@IPv6:/root/your_atri_model.zip" /path/to/your/local/deskto/
     ```

     

2. **输入服务器密码**：
   命令执行后，会提示你输入服务器的 root 用户密码。输入密码并回车，文件就会开始下载。
