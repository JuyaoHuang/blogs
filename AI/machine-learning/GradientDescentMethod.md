---
title: 梯度下降法
author: Alen
published: 2025-10-21
description: "梯度下降法及其变种的介绍和推导"
first_level_category: "人工智能"
second_level_category: "机器学习理论"
tags: ['机器学习']
draft: false
---

#### 常规梯度下降法

1. 整体梯度下降 LGD
2. 随机梯度下降 SGD
3. 批量梯度下降 BGD

#### 动量梯度下降法

#### Adam算法----应用最为广泛

Adam算法是集成了 RMSProp和 动量梯度下降法的优点，特别适用于**大多非凸优化问题（大数据集和高维空间），特别是 Transformer等现代深度学习大模型**
主要优点是：

- 使用了动量法的历史梯度加速了当前梯度的下降速度，保证初期迭代很快
- 又使用了 RMSProp的惩罚机制，极大避免了历史梯度对当前梯度下降速度的较大影响，控制了自适应学习率的增加速度。

例如，前期 Adam算法梯度下降很快，因为历史梯度值小，迭代速度快，每一步的学习率都很大（大步向山下迈）；后期迭代时，由于历史梯度影响，每一步的学习率都比初期小得多（小步走向最低处），保证不会走过最低处，避免了过拟合问题。

过拟合问题：  

这里体现为： 

1. 走到最低处时走过了，然后反复在最低处附近左右横跳，一直无法到达最低处。 对应于后期学习率较大，模型无法收敛

2. 不仅走过了最低处，还往高处不断攀登，离最低点越来越远，损失值越来越大。 对应于学习率过大，每一步的步长都特别大，落点很难控制。