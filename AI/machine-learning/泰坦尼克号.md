---
title: begin:泰坦尼克
author: Alen
published: 2025-10-14
description: "泰坦尼克号数据集实践-好的开始"
first_level_category: "AI"
second_level_category: "机器学习"
tags: ['机器学习']
draft: false
---

# 泰坦尼克号数据集实践

## 数据获取

### Seaborn库获取

```
conda install seaborn
```

加载数据集：

```python
import seaborn as sns
df = sns.load_dataset('titanic')
```

### Kaggle获取

 [Titanic - Machine Learning from Disaster](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.kaggle.com%2Fc%2Ftitanic) ， 在 "Data" 标签页下，下载 train.csv 和 test.csv 文件到本地项目文件夹。

加载数据

```python
import pandas as pd
df = pd.read_csv('train.csv')
```

## 数据预分析

### .head()：初见数据，建立直观印象

主要作用是：

- **理解特征含义**：我们可以看到列名，并猜测它们的含义。例如，survived (是否生还)，pclass (船舱等级)， sex (性别), age (年龄)，sibsp (同行的兄弟姐妹/配偶数)，parch (同行的父母/子女人数)，fare (票价)。
- **观察数据格式**：
  - survived 是 0 和 1，这是典型的**二元数值型**变量，适合做分类任务的目标。
  - sex 是 'male' 和 'female'，是**文本类别型**变量。
  - deck 列在第一行就是 NaN (Not a Number)，这是一个**缺失值**的直接信号。
- **发现潜在的冗余信息**：
  - 我们同时看到了 survived (0/1) 和 alive ('no'/'yes') 这两列。它们表达的是完全相同的信息，只是格式不同。在后续处理中，我们只需要保留一列。
  - 同样，pclass (1,2,3) 和 class ('First', 'Second', 'Third') 也是冗余的。
  - embarked ('S', 'C') 和 embark_town ('Southampton', 'Cherbourg') 也是如此。

```bash
   survived  pclass     sex   age  ...  deck  embark_town  alive  alone
0         0       3    male  22.0  ...   NaN  Southampton     no  False
1         1       1  female  38.0  ...     C    Cherbourg    yes  False
2         1       3  female  26.0  ...   NaN  Southampton    yes   True
3         1       1  female  35.0  ...     C  Southampton    yes  False
4         0       3    male  35.0  ...   NaN  Southampton     no   True
```

**初步结论**：数据集包含数值、文本、布尔等多种数据类型，存在明显的缺失值和信息冗余的列。

### info()：深入了解结构

info() 提供了数据的整体结构信息，是发现**缺失值**和**数据类型**问题的关键。

- **数据集规模**：RangeIndex: 891 entries 告诉我们这个数据集中共有 **891条记录**（代表891名乘客）。这是我们所有分析的基准。
- **数据类型 (Dtype)**：
  - object 类型通常是字符串，如 sex, embarked, who。这些在喂给机器学习模型前需要进行编码（如 **One-Hot Encoding**）。
  - int64, float64 是数值型，可以直接用于计算。
  - category 和 bool 是特殊的类型，通常也需要转换成数值。
- **发现缺失值（核心洞察）**：通过对比每一列的 Non-Null Count 和总数 891，可以精确地定位缺失数据：
  - **age**: 只有 714 non-null，说明有 891 - 714 = 177 个年龄数据是缺失的。这是一个比较严重的问题，需要后续处理（填充或删除）。
  - **embarked** 和 **embark_town**: 只有 889 non-null，说明有 891 - 889 = 2 个登船港口数据缺失。数量很少，容易处理。
  - **deck**: 只有 203 non-null，说明有 891 - 203 = 688 个船舱号数据缺失。**缺失率极高（超过77%）**，这个特征可能无法使用，后续很大概率会直接删除。

```json
RangeIndex: 891 entries, 0 to 890
Data columns (total 15 columns):
 #   Column       Non-Null Count  Dtype
---  ------       --------------  -----
 0   survived     891 non-null    int64
 1   pclass       891 non-null    int64
 2   sex          891 non-null    object
 3   age          714 non-null    float64
 4   sibsp        891 non-null    int64
 5   parch        891 non-null    int64
 6   fare         891 non-null    float64
 7   embarked     889 non-null    object
 8   class        891 non-null    category
 9   who          891 non-null    object
 10  adult_male   891 non-null    bool
 11  deck         203 non-null    category
 12  embark_town  889 non-null    object
 13  alive        891 non-null    object
 14  alone        891 non-null    bool
```

**初步结论**：deck列因缺失太多基本无用，age列有显著的缺失需要重点处理，embarked列有少量缺失可以轻松修复。

### isnull().sum()：量化缺失问题

这个命令是 info() 中关于缺失值信息的提炼和确认，让我们更直观地看到问题。

- **deck: 688**:    再次确认，deck列的缺失问题是最大的。
- **age: 177**:    确认了age是第二大缺失项。
- **embarked:  2** 和 **embark_town:  2**: 确认了这两列有少量缺失。
- **其他列: 0**: 告诉我们其他列的数据是完整的，这非常好。

```json
各列缺失值数量:
survived         0
pclass           0
sex              0
age            177
sibsp            0
parch            0
fare             0
embarked         2
class            0
who              0
adult_male       0
deck           688
embark_town      2
alive            0
alone            0
dtype: int64
```

**初步结论**：我们已经精确锁定了需要进行缺失值处理的3个主要特征：deck, age, embarked。

### describe()：洞察数值特征的分布与规律

describe() 提供了所有**数值型**列的统计摘要，这是挖掘数据规律和发现异常值的金矿。

- **survived (目标变量)**：
  - mean: 0.383838：因为生还为1，遇难为0，所以均值就是**生还率**。我们可以得出一个非常重要的基线结论：**数据集中总体的生还率约为 38.4%**。
- **pclass (船舱等级)**：
  - mean: 2.3：平均等级是2.3，说明大部分乘客（超过一半）都在三等舱（因为中位数50%是3.0）。
- **age (年龄)**：
  - count: 714：再次确认了非空值的数量。
  - mean: 29.7, 50% (median): 28.0：乘客的平均年龄约30岁，中位数和均值很接近，说明年龄分布可能不会太偏斜。
  - min: 0.42, max: 80.0：乘客年龄跨度很大，从婴儿到老人都有。
- **sibsp & parch (家庭成员数)**：
  - 75%: 1.000000 (sibsp) 和 75%: 0.000000 (parch)：这两个值告诉我们，**超过75%的乘客没有同行父母/子女，且大部分乘客的同行兄弟姐妹/配偶数不超过1人**。结合起来看，大多数人是独自或以小家庭形式出行。
- **fare (票价)**：
  - mean: 32.2 vs 50% (median): 14.45：**均值远大于中位数**，这是一个强烈的信号，表明票价分布是严重右偏的。
  - std: 49.7：标准差非常大，甚至超过了均值，说明票价的波动范围极广。
  - max: 512.3：存在极高的票价，这很可能是异常值或极端值，是那些豪华头等舱的票价，它们拉高了整体的平均值。

```json
          survived      pclass         age       sibsp       parch        fare
count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000
mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208
std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429
min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000
25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400
50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200
75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000
max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200
```



### **综合分析与下一步行动计划**

通过这四个命令，我们几乎可以为后续的数据清洗和特征工程制定一个完整的计划：

1. **数据清洗**:
   - **删除冗余列**：从 survived/alive, pclass/class, embarked/embark_town 中各选一列保留。
   - **处理缺失值**：
     - **删除deck列**，因为缺失值太多。
     - **填充age列**，可以使用均值、中位数填充，或者更高级的方法（如根据头衔Mr., Mrs.的平均年龄来填充）。
     - **填充embarked列**，可以使用最常见的登船港口来填充。
2. **特征工程**:
   - 将sex、embarked等文本类别型特征转换为数值（如One-Hot Encoding）。
   - 可以考虑将sibsp和parch合并成一个新的特征，如family_size（家庭总人数）。
   - 由于fare和age的分布和范围差异很大，后续建模时需要进行**特征缩放（标准化或归一化）**。
3. **初步假设建立**:
   - 生还率可能与pclass和fare有关（高等级/高票价的乘客是否生还率更高？）。
   - 生还率可能与sex有关（女性的生还率是否更高？）。
   - 生还率可能与age有关（儿童的生还率是否更高？）。

## 数据清洗

### 数据冗余处理

使用 .info() 查看：

```bash
#   Column       Non-Null Count  Dtype
---  ------       --------------  -----
 0   survived     891 non-null    int64 ------- ‘1’代表活着，'0'代表死亡
 1   pclass       891 non-null    int64 -----船舱等级 1、2、3
 2   sex          891 non-null    object  ---- 'male' or 'female'
 3   age          714 non-null    float64
 4   sibsp        891 non-null    int64 -----父母数
 5   parch        891 non-null    int64  ----子女数
 6   fare         891 non-null    float64 ----费用
 7   embarked     889 non-null    object  ---- 与embark_town重复，舍弃
 8   class        891 non-null    category ------ 船舱等级 first、second、third
 9   who          891 non-null    object   ------ 'man' or 'woman'，舍弃
 10  adult_male   891 non-null    bool   ------- 是否成年 True or False
 11  deck         203 non-null    category ------样本缺失太多，舍弃
 12  embark_town  889 non-null    object
 13  alive        891 non-null    object  ---- 与survived重复，舍弃
 14  alone        891 non-null    bool ---- 是否一个人来的
```

可知需要除去几个冗余数据，优先保留数值类，滤除对象，以减少 后续文本特征转换数值的操作：One-Hot Encoding

**冗余列**：

1. alive
2. who
3. class
4. embarked

```python
df = df.drop(['alive','deck','who','embarked','class'],axis=1) # axis保证删除的是列
```

```python
RangeIndex: 891 entries, 0 to 890
Data columns (total 11 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   survived     891 non-null    int64  
 1   pclass       891 non-null    int64  
 2   sex          891 non-null    object 
 3   age          891 non-null    float64
 4   sibsp        891 non-null    int64  
 5   parch        891 non-null    int64  
 6   fare         891 non-null    float64
 7   adult_male   891 non-null    bool   
 8   embark_town  891 non-null    object 
 9   alone        891 non-null    bool   
 10  family       891 non-null    int64  
```

---

**合并数据**

观察发现：`sibsp`和`parch`意思实际上是同一个，可以合并为 `family` 列，代表家庭成员数。而保留alone是因为方便后续做数据分析：生还率是否与单独一人有关?

```python
df['family'] = df['sibsp'] + df['parch'] + 1
df = df.drop(['sibsp','parch'],axis=1)
```

```bash
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   survived     891 non-null    int64  -- 是否活着：0=dead,1=survived
 1   pclass       891 non-null    int64  -- 船舱等级：1、2、3 
 2   sex          891 non-null    object -- 性别：'male' or 'female'
 3   age          891 non-null    float64 -- 年龄
 4   fare         891 non-null    float64 -- 费用
 5   adult_male   891 non-null    bool   -- 是否成年：'True' or 'False'
 6   embark_town  891 non-null    object -- 登录港口
 7   alone        891 non-null    bool   -- 是否一个人：'True' or 'False'
 8   family       891 non-null    int64  -- 家庭成员数：包括自己
```



### 缺失值处理

使用指令 'df.isnull().sum()' 得到数据集缺失情况

```bash
各列缺失值数量:
survived         0
pclass           0
sex              0
age            177
sibsp            0
parch            0
fare             0
embarked         2
class            0
who              0
adult_male       0
deck           688
embark_town      2
alive            0
alone            0
```

发现：

1. deck 缺失过多，根本无法使用
2. age 缺失 177条
3. embark_town 缺失 2条

#### 1.计算缺失率

1. 获取缺失条数

   ```python
   age_null = df['age'].isnull().sum()
   ```

2. 获取总条数

   获取总共有多少条数据有几种办法

   1. 使用 python 的 len（）直接作用 DataFrame/Series 对象，返回行数

      ```python
      a = len(df['age'])
      # 891
      ```

   2. 使用下标索引方式，.index[-1] 返回最后的下标索引

      ```python
      age_cnt = df['age'].index[-1]
      print(age_cnt)
      # 890
      ```

   3. 使用 .shape[0] 获取行数

      ```python
      age_cnt = df['age'].shape
      # (891,)
      age_cnt = df['age'].shape[0]
      print(age_cnt)
      # 891
      ```

   4. 使用 .tail(1)获得最后一条数据

      ```python
      df['age'].tail(1)
      # 890    32.0
      a = df['age'].tail(1).index[0]
      print(a)
      # 890
      ```

   5. 注意： .iloc[n] 获取的是第n条数据的**值**，不是索引

3. 计算单列数据的缺失率

   ```python
   age_null = df['age'].isnull().sum()
   age_cnt = df['age'].index[-1] + 1
   print(f'缺失比：{(age_null/age_cnt)*100:.3f}%')
   # 缺失比：19.865%
   ```

4. 一次性计算所有列的数据的缺失率

   ```python
   val_null = df.isnull().sum()
   total_cnt = df.shape[0] # [0] 代表行数，[1] 代表列数
   print(f'总数据条数{total_cnt}')
   miss_rate = (val_null/total_cnt)*100
   print(f'各列的缺失率(%):\n {miss_rate[miss_rate>0].sort_values(ascending=False)}')
   # 总数据条数 891
   # 各列的缺失率(%):
   # age            19.865320
   # embark_town     0.224467
   ```

可得：

| deck缺失率 | age缺失率 | embark_town缺失率 |
| :--------: | :-------: | :---------------: |
|  77.217%   |  19.865%  |      0.224%       |

=> deck 数据无法使用，age 和 embark_town 有使用的价值

#### 2.填补缺失值

1.  age 是数值类型，使用均值受到的影响很大，填补后会极大影响原本的数据特征，故采用中位数

   ```python
   age_median = df['age'].median()
   df['age'] = df['age'].fillna(age_median)
   ```

2. embark_town 是分类型特征（地点），使用众数填充更合适

   ```python
   e_t_mode = df['embark_town'].mode()
   # print(e_t_mode)
   # 0    Southampton
   df['embark_town'] = df['embark_town'].fillna(e_t_mode[0]) # 选择序列号0的元素
   print(df.isnull().sum())
   ```

3. 填充完毕：

   ```python
   survived       0
   pclass         0
   sex            0
   age            0
   sibsp          0
   parch          0
   fare           0
   class          0
   adult_male     0
   embark_town    0
   alone          0
   family         0
   ```

   

---

**数据填充的选择**

​	**选择的填充方法，必须最符合该特征的数据类型和其分布特点，以最大程度地减少对原始数据信息的扭曲**

1. **均值**

   例如 age 是数值型数据，并且其分布可能受到异常值（极端值）的影响，中位数对异常值不敏感，是比均值更稳健、更安全的“中心”衡量标准。

   - 优点:     保持了数据集的整体均值不变。
   - 缺点:
     - 降低了数据的方差：因为用一个相同的值替换了许多不同的未知值，这使得数据的波动性（方差）减小了。
     - 对异常值敏感：如果数据中有极端值，均值会被拉高或拉低，用这个被污染的均值去填充可能会引入偏差。
     - 忽略了特征之间的相关性。

2. **中位数**

   - 方法: 

     ```python
     df['age'] = fillna(df['age'].median())
     ```

   - 适用场景:     

     ​	当数据分布是**偏斜的**或存在异常值时，中位数比均值更具代表性，是更稳健的选择。因为它**不受或很少受**数据集两端的极端值影响

3. **众数**

   - 方法: (注意：.mode()返回一个Series，所以要取第一个元素 [0])

     ```python
     df['embarked'] = fillna(df['embarked'].mode()[0]) 
     ```

   - 适用场景: 

     ​	主要用于填充**分类型特征**，例如 embarked（登船港口）。用出现次数最多的港口来填充缺失的两个值是合理的。

### 数据类型转换

观察 .info 的输出，发现有一些列数据类型，模型无法使用：布尔、对象object

```bash
RangeIndex: 891 entries, 0 to 890
Data columns (total 11 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   survived     891 non-null    int64  
 1   pclass       891 non-null    int64  
 2   sex          891 non-null    object ---> 1111
 3   age          891 non-null    float64
 4   sibsp        891 non-null    int64  
 5   parch        891 non-null    int64  
 6   fare         891 non-null    float64
 7   adult_male   891 non-null    bool   ---> 2222
 8   embark_town  891 non-null    object ---> 3333
 9   alone        891 non-null    bool   ---> 4444
 10  family       891 non-null    int64  
```

因此需要对其进行类型转换

#### 1.布尔类型转换

pandas库有一个专门的函数做这件事：.astype()

将布尔类型转换为`int64`的类型

```python
df['alone'] = df['alone'].astype(int)
df['adult_male'] = df['adult_male'].astype(int)
print(df.info())
```

如果不记得，手动创造函数使用：

```python
def turn_type(val):
    if val:
        return 1
    elif not val:
        return 0

df['alone'] = df['alone'].apply(turn_type)
df['adult_male'] = df['adult_male'].apply(turn_type)
print(df['alone'].value_counts())
print(df['adult_male'].value_counts())
```

```bash
alone
1    537
0    354
Name: count, dtype: int64
adult_male
1    537
0    354
Name: count, dtype: int64
```

#### 2.One-Hot Encoding

### 文本类别型特征转换为数值

一般采用：One-Hot Encoding 的方法

### 数据标准化

#### 1.定义

数据标准化，也称为 Z-score 标准化，是一种将数据按比例缩放的方法，**使处理后的数据具有零均值（mean=0）和单位标准差（standard deviation=1）的特性**。

背后的数学公式很简单：
$$
z = \frac{x-\mu}{\sigma}
$$
其中：

- $x$ 是原始数据点
- $\mu$ ：该特征所有数据的平均值
- $\sigma$ ：该特征所有数据的标准差
- $z$ ：标准化之后的新数据点

#### 2.目的

在泰坦尼克数据集中，age 的范围是 0-80，而 fare 的范围是 0-512。

如果直接将这些数据喂给某些机器学习模型（例如逻辑回归、支持向量机SVM、神经网络等），模型可能会**错误**地认为 fare 这个特征比 age 更重要，仅仅因为它本身的数值更大。

标准化的目的就是消除这种数值范围（量纲）带来的影响，让所有特征都站在“同一起跑线”上，从而：

1. **提升模型收敛速度**：    对于使用梯度下降的算法，标准化后的数据可以帮助算法更快地找到最优解。
2. **提高模型精度**：    避免模型被某些数值范围过大的特征所主导，从而更公平地学习所有特征的贡献。
3. **适用于特定模型**：    对于依赖距离计算的模型（如K近邻KNN、SVM）或依赖方差的算法（如主成分分析PCA），标准化是必需的。

#### 3.具体操作

最常用方法是使用 **Scikit-learn** 库中的 StandardScaler

**在此之前**：需要先将数据集划分为 测试集 和 训练集，**只在训练集上进行 fit**（学习均值和标准差），再用这个学习到的规则去 transform（应用公式）训练集和测试集。

这是为了防止**数据泄露**。模型的任何预处理步骤都应该是从训练数据中学习到的，测试集应该被当作是模型从未见过的“未来数据”。如果在整个数据集上进行标准化，那么测试集的信息（它的均值和标准差）就已经“泄露”给了训练过程

1. 选择要标准化的数值列

   ```python
   from sklearn.model_selection import train_test_split
   from sklearn.preprocessing import StandardScaler
   
   numeric_features = ['age', 'fare']
   sub_df = df[numeric_features].copy() # 使用 .copy() 复制该 DF
   ```

2. 处理缺失值

3. 划分测试集和训练集

   ```python
   X = sub_df
   y = df['survived']
   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
   ```

4. 创建 StandardScaler 实例

   ```python
   scaler = StandardScaler()
   ```

5. 在训练集上进行 fit 和 transform

   **fit()**:     计算训练集的均值(μ)和标准差(σ)

   **transform()**:     使用计算出的μ和σ，对训练集应用标准化公式

   ```python
   X_train_scaled = scaler.fit_transform(X_train)
   ```

6. 测试集上只进行 transform

   使用在训练集上学习到的μ和σ来标准化测试集

   ```python
   X_test_scaled = scaler.transform(X_test)
   ```

7. 查看标准化后的结果

   scaler返回的是Numpy数组，将其转回DataFrame以便查看

   ```python
   X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=numeric_features, index=X_train.index)
   ```

#### 另一种方法：归一化

除了标准化，还有一种常用的缩放方法叫**归一化**，它将数据缩放到一个固定的范围，通常是。Scikit-learn 中对应的工具是 MinMaxScaler。

**何时选择？**

- **标准化**：

  ​	是最常用的方法。适用于数据基本符合正态分布，或者想使用那些对特征尺度敏感但对数据分布没有特定要求的算法时。

- **归一化**：

  ​	需要将数据限定在一个明确的范围内（如图像处理中的像素值0-255），或者数据分布非常不规则且含有许多离群值时，它可能不是最佳选择，因为离群值会严重影响最大值和最小值。



## 数据绘图

