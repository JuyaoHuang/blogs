---
title: "ç®€å•çš„åç«¯æ„å»º"
published: 2025-12-10
description: "ä½¿ç”¨ fastAPI å®Œæˆ llm è°ƒç”¨çš„ç®€å•åç«¯å®ç°"
tags: ['AI agent']
first_level_category: "é¡¹ç›®å®è·µ"
second_level_category: "agentæ­å»º"
draft: false
---

## é¡¹ç›®äºŒç®€ä»‹

**æ„å»ºä¸€ä¸ªé«˜çº§APIå°è£…å™¨**ï¼š

ä½¿ç”¨FastAPIï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„åç«¯æœåŠ¡ã€‚å®ƒæ¥æ”¶ä¸€ä¸ªä»»åŠ¡æè¿°ï¼ˆæ¯”å¦‚â€œæ€»ç»“è¿™æ®µæ–‡å­—â€æˆ–â€œæŠŠè¿™æ®µè‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡â€ï¼‰ï¼Œç„¶ååœ¨å†…éƒ¨æ„å»ºä¸€ä¸ªé«˜è´¨é‡çš„ Promptï¼Œè°ƒç”¨ LLM APIï¼Œæœ€åå°† LLM è¿”å›çš„å¹²å‡€ç»“æœä½œä¸º API çš„å“åº”è¿”å›
ç›®çš„: å°† LLMçš„ å¼ºå¤§èƒ½åŠ›ï¼Œå°è£…æˆå¯ä»¥è½»æ¾è°ƒç”¨çš„ã€å¯é çš„åç«¯æœåŠ¡

## è§£å†³æ–¹æ¡ˆ

### ç®€å•å®ç°

ä½¿ç”¨ fastAPI å°è£…ä¸€ä¸ª API ç«¯ç‚¹ï¼Œè¯¥ç«¯ç‚¹å®ç° `æ¥æ”¶æ–‡æœ¬ -> æ„å»º prompt -> è°ƒç”¨ llm -> è¿”å›æ¸…æ´—å¥½çš„ã€ç¬¦åˆçº¦æŸçš„ JSON å†…å®¹`ã€‚

> æ³¨æ„ï¼Œæ‰€æœ‰å†…å®¹éƒ½åœ¨ä¸€ä¸ª API é‡Œå®ç°ï¼Œå®é™…ä¸Šè¿™è¿‡äºè‡ƒè‚¿äº†ã€‚
> å¯ä»¥å°†â€œæ„å»º promptâ€å’Œâ€œllm è°ƒç”¨â€è¿™ä¸¤éƒ¨åˆ†ç»™æŠ½è±¡ä¸ºç‹¬ç«‹çš„æ¨¡å—ã€‚ä¾‹å¦‚ prompts module, stateless llm moduleã€‚ä¹‹åå†æœ‰å…¶ä»–æ–°çš„ç«¯ç‚¹éœ€è¦å®ç°ä¸Šé¢çš„æµç¨‹ï¼Œåªéœ€è¦ä¼ å…¥éœ€è¦çš„å‚æ•°å³å¯ï¼Œä¸éœ€è¦å†™é‡å¤çš„ä»£ç ã€‚

### ç»“æ„åŒ–å®ç°

> ä¸ºä¿è¯æ–‡ç« é˜…è¯»çš„æµç•…æ€§ï¼Œè¯¥éƒ¨åˆ†åç»­ç»™å‡ºã€‚

å°†ç®€å•å®ç°çš„æµç¨‹çš„ä¸€äº›å†…å®¹æŠ½è±¡å°è£…ä¸ºæ¨¡å—è¿›è¡Œè°ƒç”¨ã€‚

## ç®€å•æ–¹æ¡ˆçš„å®ç°æ­¥éª¤

**1. åˆå§‹åŒ– fastAPI åº”ç”¨**

```python
import os
import json
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from openai import OpenAI
from dotenv import load_dotenv
# ç”±äºå›½å†…å¯èƒ½å­˜åœ¨çš„ CDN æµé‡é™åˆ¶ï¼Œæ•…æ‰‹åŠ¨å®šä¹‰ fastapi è‡ªå¸¦çš„ swagger ui é¡µé¢çš„é™æ€èµ„æºåŠ è½½
# ä»¥ä¸‹çš„åŒ…çš„ä½œç”¨æ˜¯ä½¿ç”¨å›½å†…é•œåƒæºåŠ è½½ fastAPI è‡ªå¸¦çš„ seagger ui é¡µé¢
from fastapi.openapi.docs import get_swagger_ui_html
from fastapi.openapi.docs import get_redoc_html

load_dotenv()
app = FastAPI(title="Atri Translator", docs_url=None, redoc_url=None)
```

**2. ä½¿ç”¨å›½å†…é•œåƒCDNåŠ é€Ÿç«™åŠ è½½ swagger ui**

```python
# Add swagger-ui mirror
@app.get("/docs", include_in_schema=False)
async def custom_swagger_ui_html():
    return get_swagger_ui_html(
        openapi_url=app.openapi_url,
        title=app.title + " - Swagger UI",
        oauth2_redirect_url=app.swagger_ui_oauth2_redirect_url,
        swagger_js_url="https://cdn.bootcdn.net/ajax/libs/swagger-ui/5.29.1/swagger-ui-bundle.js",
        swagger_css_url="https://cdn.bootcdn.net/ajax/libs/swagger-ui/5.29.1/swagger-ui.css",
    )
@app.get("/redoc", include_in_schema=False)
async def redoc_html():
    return get_redoc_html(
        openapi_url=app.openapi_url,
        title=app.title + " - ReDoc",
        redoc_js_url="https://unpkg.com/redoc@next/bundles/redoc.standalone.js",
    )
```

**3. åˆå§‹åŒ– llm å®¢æˆ·ç«¯**

```python
client = OpenAI(
    api_key=os.environ.get("ALIYUN_API_KEY"),
    base_url=os.environ.get("ALIYUN_BASE_URL"),
)
```

**4. å®šä¹‰ pydantic æ¨¡å‹**

ä»¥æ„å»ºä¸€ä¸ªç¿»è¯‘å™¨ä¸ºä¾‹ï¼Œå®šä¹‰ä¸¤ä¸ªæ•°æ®æ¨¡å‹ï¼šä¸€ä¸ªç”¨äºå‘é€æ¶ˆæ¯ï¼Œä¸€ä¸ªç”¨äºæ¥æ”¶è¿”å›çš„æ¶ˆæ¯ã€‚

```python
class TranslateRequest(BaseModel):
    text: str = Field(..., description="éœ€è¦ç¿»è¯‘çš„åŸæ–‡", min_length=1)
    target_lang: str = Field("English", description="ç›®æ ‡è¯­è¨€ï¼Œé»˜è®¤ä¸ºè‹±è¯­")
class TranslateResponse(BaseModel):
    original_text: str
    translated_text: str
    detected_language: str = "unknown"
```

**5. å¤„ç†æœåŠ¡è¯·æ±‚**

ä½¿ç”¨ fastAPI çš„è·¯ç”±å®šä¹‰å’Œå¼‚æ­¥è¯·æ±‚æ–¹å¼ï¼Œå®Œæˆæ ¸å¿ƒçš„å‘é€-æ¥æ”¶é€»è¾‘ã€‚

```python
# å®šä¹‰æ¥æ”¶çš„ API è¯·æ±‚çš„è·¯ç”±
@app.post("/api/translate", response_model=TranslateResponse)
async def translate(request: TranslateRequest):
    """
    access text -> build Prompt -> llmcalling -> return JSON well-clear
    :param request:
    :return: JSON well-clear
    """
```

**æ„å»ºç³»ç»Ÿ prompt**ï¼š

```python
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç²¾é€šå¤šå›½è¯­è¨€çš„èµ„æ·±ç¿»è¯‘å¼•æ“ã€‚
    ä»»åŠ¡ï¼š
    1. å°†ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ç¿»è¯‘æˆ {request.target_lang}ã€‚
    2. è‡ªåŠ¨æ£€æµ‹åŸæ–‡çš„è¯­è¨€ã€‚
    3. å¿…é¡»ä¸¥æ ¼ä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - translated_text: ç¿»è¯‘åçš„å†…å®¹
        - source_lang: åŸæ–‡çš„è¯­è¨€ï¼ˆå¦‚ Chinese, English, Frenchï¼‰
    ç¤ºä¾‹ï¼š
    {{
        "translated_text": "hello",
        "source_lang": "en"
    }}
    """
```

**æ„å»º API è¯·æ±‚**

```python
    response = client.chat.completions.create(
        model="qwen3-max",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": request.text},
        ],
        temperature=0.3,
    )
```

**æ¥æ”¶è¿”å›å†…å®¹å¹¶è½¬ä¸ºå­—å…¸**ï¼ˆå› ä¸ºè¿”å›çš„æ˜¯ JSON ï¼‰

```python
    content = response.choices[0].message.content
    # turn json into dict
    data = json.loads(content)
```

**å°†æ‹¿åˆ°çš„å­—å…¸å†…å®¹è¿”å›ç»™å®¢æˆ·ç«¯**

```python
      # return to client
      return TranslateResponse(
          original_text=request.text,
          translated_text=data.get("translated_text", "Fail to translate"),
          detected_language=data.get("source_lang", "unknown"),
      )
```

**å®šä¹‰æ ¹ç›®å½•è·¯ç”±**ï¼ˆç”¨äºæµ‹è¯• UI ç•Œé¢æ­£å¸¸æ‰“å¼€ï¼‰

```python
@app.get("/")
async def root():
    return {"message": "AI server is running! please open thr link /docs to see docs."}
```

**è¿è¡ŒæŒ‡ä»¤**

```bash
uvicorn main:app --reload
```

æˆ–è€…

```bash
fastapi dev main.py --port 8000
```

ç„¶åæ‰“å¼€ `http://127.0.0.0:8000`ã€‚

> å¦‚æœå‡ºç°é•¿æ—¶é—´æ‰“ä¸å¼€ UI é¡µé¢çš„æƒ…å†µï¼Œæœ‰å¯èƒ½æ˜¯ç«¯å£è¢«å ç”¨ã€‚ä½¿ç”¨ `--port` æŒ‡ä»¤åˆ‡æ¢ç«¯å£ã€‚

## æ¶æ„é‡æ„

å°±åƒå‰æ–‡è¯´çš„ï¼Œå¤„ç† `æ¥æ”¶è¯·æ±‚ -> æ„å»º prompt -> llm è°ƒç”¨ -> è§£æå“åº” -> è¿”å›éœ€è¦çš„æ•°æ®ç»“æ„` çš„æ‰€æœ‰ä»£ç å…¨éƒ¨å †ç§¯åœ¨ä¸€ä¸ª API ç«¯ç‚¹é‡Œã€‚è¿™æ ·ä»£ç å®¹æ˜“è‡ƒè‚¿ï¼Œä¸”æœ‰å‡ ä¸ªéƒ¨åˆ†æ˜¯å¯ä»¥**ä»£ç å¤ç”¨**çš„ã€‚ä¾‹å¦‚ prompt æ„å»ºã€llm è°ƒç”¨å’Œè§£æå“åº”ã€‚

å› æ­¤å¯ä»¥åšå‡ºä»¥ä¸‹é‡æ„ï¼š
1. å€Ÿç”¨ MVC è®¾è®¡æ€è·¯ï¼Œå°†é€»è¾‘åˆ’åˆ†ä¸ºï¼šæœåŠ¡å±‚ã€é…ç½®å±‚ã€‚
2. é…ç½®å±‚ï¼šè¿›è¡Œ fastapi çš„é…ç½®ï¼Œä¾‹å¦‚æ ‡é¢˜ã€æè¿°ã€å¯åŠ¨ç«¯å£ã€å¯åŠ¨å…¥å£ç­‰
3. æœåŠ¡å±‚ï¼šä¸šåŠ¡å¤„ç†ã€promptå·¥å‚ã€Pydanticæ¨¡å‹å®šä¹‰ã€llm è°ƒç”¨

```bash
project_two
 â”£ config
 â”ƒ â”£ config.py
 â”£ services
 â”ƒ â”£ llmcalling.py 
 â”ƒ â”£ prompt_factory.py 
 â”ƒ â”£ schema.py
 â”ƒ â”£ server.py
 â”£ main.py
```

è¿™æ ·çš„ç»“æ„å¢åŠ äº†å¯æ‰©å±•æ€§å’Œç¨³å®šæ€§ã€‚
> å®é™…ä¸Šé…ç½®æ–‡ä»¶åœ¨ /project_two ä¸‹åˆ›å»º .env ä½œä¸ºå­é…ç½®æ–‡ä»¶ï¼ˆé¡¹ç›®æ ¹ç›®å½•æœ‰æ ¹`.env`ï¼‰ï¼Œ
> ä»ç¯å¢ƒé‡Œè¯»å–æ‰€æœ‰é…ç½®æ‰æ˜¯ç”Ÿäº§ç¯å¢ƒçš„æ ‡å‡†åšæ³•

è¿™æ ·å¯¹äºä¸šåŠ¡å¤„ç†ä»£ç éƒ¨åˆ†ï¼ˆAPI ç«¯ç‚¹ï¼‰ä¸å¿…å†å†™å¤šä½™çš„ä»£ç ä»¥åŠè€ƒè™‘ llm è°ƒç”¨ã€prompt æ„å»ºã€ç»“æ„è§£æç­‰éƒ¨åˆ†çš„ä»£ç ï¼Œç›´æ¥è°ƒç”¨ç›¸å…³çš„æ–¹æ³•å³å¯ã€‚

**å¯¹äº `server.py`**ï¼Œå…¶åŸæœ¬çš„ä¸šåŠ¡å¤„ç†é€»è¾‘ä¸å˜ï¼Œä½†æ˜¯ä»£ç å·²è¢«ç²¾ç®€ä¸ºï¼š

```python
@app.post("/api/translate", response_model=TranslateResponse)
async def translate(request: TranslateRequest):
    """
    access text -> build Prompt -> llmcalling -> return JSON well-clear
    :param request:
    :return: JSON well-clear
    """
     system_prompt = PromptFactory.get_translate_prompt(request.target_lang)

     messages = [
         {"role": "system", "content": system_prompt},
         {"role": "user", "content": request.text},
     ]
     content = llmcalling("qwen3-max", messages, 0.3)

     # turn json into dict
     data = json.loads(content)

     # return to client
     return TranslateResponse(
         original_text=request.text,
         translated_text=data.get("translated_text", "Fail to translate"),
         detected_language=data.get("source_lang", "unknown"),
     )
```
åªéœ€æ„å»ºå¥½ä¼ å…¥æ¨¡å‹çš„ `message` éƒ¨åˆ†ï¼Œå†å°†ç»“æœè¿”å›å³å¯ã€‚ä»£ç é«˜åº¦ç²¾ç®€å’Œå¯è¯»ã€‚

è€Œéœ€è¦æ„å»ºæ–°çš„ï¼ˆä¾‹å¦‚è§’è‰²æ‰®æ¼”ç­‰å’Œ llm äº’åŠ¨ï¼‰API ç«¯ç‚¹æ—¶ï¼Œä¹Ÿæ˜¯ä¸€æ ·çš„å¤„ç†é€»è¾‘ï¼Œåªéœ€è¦åœ¨ prompt å·¥å‚é‡Œæ–°å¢ç³»ç»Ÿ promptå³å¯ã€‚

**é‡æ„åçš„ä»£ç å¯åœ¨æ­¤ commit æŸ¥çœ‹**ï¼šhttps://github.com/JuyaoHuang/AI-agent/pull/9/commits/9544f2f8c77739cce1cef9a379c2463cc5b7555f

> è¯¥ commit è¿˜æ²¡æœ‰å°† llmcalling æŠ½è±¡å‡ºæ¥ã€‚

## å®è·µæ‰©å±•ä¸€

Issue é“¾æ¥ï¼šhttps://github.com/JuyaoHuang/AI-agent/issues/8

**é¡¹ç›® 2 çš„åŸºæœ¬éœ€æ±‚å·²å®Œæˆ**ã€‚é¡¹ç›®ç»“æ„å·²æˆåŠŸé‡æ„ä¸ºæ ‡å‡†çš„ MVC æ¶æ„ã€‚åŸºäºæ­¤åšå®çš„åŸºç¡€ï¼Œæˆ‘è®¡åˆ’è¿›ä¸€æ­¥æ‰©å±•é¡¹ç›®çš„åŠŸèƒ½ã€‚

### ç›®æ ‡

æ­¤é—®é¢˜è·Ÿè¸ªä»¥ä¸‹æ‰©å±•çš„å®ç°ï¼š

- å®ç° /api/summary ç«¯ç‚¹ï¼šåˆ›å»ºä¸€ä¸ªç”¨äºæ–‡æœ¬æ‘˜è¦çš„æ–° API è·¯ç”±ï¼Œåˆ©ç”¨ PromptFactory ç®¡ç†æ‘˜è¦ç›¸å…³çš„æç¤ºï¼Œä¸ºæ­¤ä»»åŠ¡å»ºç«‹ä¸€ä¸ªæ–°çš„ LLM è°ƒç”¨æµç¨‹ã€‚

- å¼€å‘å®¢æˆ·ç«¯æ¨¡æ‹Ÿè„šæœ¬ï¼šç¼–å†™ä¸€ä¸ª Python è„šæœ¬æ¥æ¨¡æ‹ŸçœŸå®ç”¨æˆ·çš„è¯·æ±‚ï¼Œä»å®¢æˆ·ç«¯çš„è§’åº¦éªŒè¯ API çš„è¡Œä¸ºå’Œæ€§èƒ½ã€‚


- å…¨æ ˆé›†æˆï¼ˆStreamlitï¼‰ï¼šå°†ç°æœ‰çš„ Streamlit å‰ç«¯ï¼ˆæ¥è‡ªé¡¹ç›® 1ï¼‰ä¸æ–°æ„å»ºçš„ FastAPI åç«¯é›†æˆï¼Œå®ç°æ— ç¼çš„å‰åç«¯é€šä¿¡ã€‚


### å®æ–½æ–¹æ¡ˆåŠæŠ€æœ¯ç»†èŠ‚

- **é‡æ„ LLM æœåŠ¡**ï¼šå°†æ ¸å¿ƒ LLM è°ƒç”¨é€»è¾‘æŠ½è±¡ä¸ºå¯é‡ç”¨çš„æœåŠ¡æ–¹æ³•/ç±»ï¼Œä»¥å‡å°‘ä»£ç é‡å¤å¹¶æé«˜å¯ç»´æŠ¤æ€§ã€‚
- **å®¢æˆ·ç«¯é€»è¾‘**ï¼šä½¿ç”¨ Python requests åº“å¤„ç† API è°ƒç”¨å¹¶è§£æè¿”å›çš„ JSON æ•°æ®ç»“æ„ã€‚
- **å‰ç«¯æ¸²æŸ“**ï¼šåœ¨ Streamlit UI ä¸Šæ¸²æŸ“è§£æåçš„ API å“åº”ã€‚
- ç›®æ ‡ï¼šå®ç°æµå¼è¾“å‡ºï¼ˆå¦‚é€‚ç”¨ï¼‰ä»¥æå‡ç”¨æˆ·ä½“éªŒã€‚

### å®ç°ä»£ç 

#### **new prompt æ„å»º**

```python
@staticmethod
def get_summary_prompt(word_limit: int=100) -> str:
    """
    Generate summary task system prompt
    :param word_limit:
    :return:
    """
    return f"""
    # Role
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ç« æ‘˜è¦åŠ©æ‰‹ï¼Œæ“…é•¿å°†é•¿æ–‡æœ¬å†…å®¹åšæ€»ç»“ï¼Œæ€»ç»“çš„å†…å®¹ç²¾å‡†ã€å‘¨åˆ°ï¼Œ
    æœ€å¤§åŒ–ä¿ç•™äº†é•¿æ–‡æœ¬çš„ä¿¡æ¯ã€‚
    # Task
    - è¯·å°†æ–‡ç« æ€»ç»“åœ¨ {word_limit} å­—ä»¥å†…
    - å°†æ–‡ç« çš„å†…å®¹æå–å‡ºæ•°ä¸ªå…³é”®è¯ä½œä¸º tags
    # Format
    å¿…é¡»è¿”å› JSON æ ¼å¼ï¼š{{ "summary": "...", "tags": [] }}
    # Example
    {{
        "summary": "è¿™æ˜¯ä¸€ç¯‡ç§‘æŠ€æ‚å¿—æ–‡ç« ...",
        "tags":['AI', 'è‹±ä¼Ÿè¾¾', 'ç§‘æŠ€']
    }}
    """
```

**æ¨¡å‹æ„å»º**

```python
class SummaryRequest(BaseModel):
    text: str = Field(..., description="éœ€è¦æ€»ç»“çš„é•¿æ–‡æœ¬", min_length=1)
    word_limit : int = Field(100, description="é™åˆ¶çš„å­—æ•°ï¼Œé»˜è®¤ 100 å­—")


class SummaryResponse(BaseModel):
    summary: str
    tags: list[str] = Field(...)
```

#### **API ç«¯ç‚¹æ„å»º**

```python
# è¾…åŠ©å‡½æ•°ï¼šæ¸…æ´—æ¨¡å‹å¯èƒ½è¿”å›çš„ ```json ä»£ç å—æ ‡æ³¨
def clean_json_string(text:str) -> str:
    """æ¸…ç†æ¨¡å‹è¿”å›çš„å¯èƒ½å­˜åœ¨çš„ markdown æ ‡è®°:```json """
    text = re.sub(r"```json\s*", "", text)
    text = re.sub(r"```", "", text)
    return text.strip()
@app.post("/api/summary", response_model=SummaryResponse)
async def summary(request: SummaryRequest):
    """å¤„ç†æ€»ç»“é•¿æ–‡æœ¬è¯·æ±‚çš„ç«¯ç‚¹"""
    system_prompt = PromptFactory.get_summary_prompt(250)
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": request.text},
    ]
    content = LLMCalling.llmcalling("qwen3-max", messages, 0.5)
    # æ¸…æ´—æ½œåœ¨çš„ json mdæ ‡æ³¨
    content = clean_json_string(content)
    data = json.loads(content)

    return SummaryResponse(
        summary=data.get("summary", "Fail to summary"),
        tags=data.get("tags",[])
    )
```

#### **åˆ›å»ºæµ‹è¯•è„šæœ¬**

```python
import requests
def translate_test(text: str, target_lang: str):
    url = "http://127.0.0.1:8028/api/translate"
    payload = {
        "text": text,
        "target_lang": target_lang
    }

    print("æ­£åœ¨è°ƒç”¨ç¿»è¯‘ API...")
    response = requests.post(url, json=payload)

    if response.status_code == 200:
        data = response.json()
        print(f"ç¿»è¯‘ç»“æœ: {data['translated_text']}")
        print(f"è¯†åˆ«è¯­è¨€: {data['detected_language']}")
    else:
        print("è°ƒç”¨å¤±è´¥:", response.text)
        
def summary_test(text: str, word_limit: int):
    url = "http://127.0.0.1:8028/api/summary"
    pay_load = {
        "text": text,
        "word_limit": word_limit
    }
    print("æ­£åœ¨è°ƒç”¨æ€»ç»“æ‘˜è¦ API...")
    response = requests.post(url, json=pay_load)
    if response.status_code == 200:
        data = response.json()
        print(f"æ‘˜è¦ï¼š{data['summary']}")
        print(f"æ ‡ç­¾ï¼š{data['tags']}")
    else:
        print("è°ƒç”¨å¤±è´¥:", response.text)

if __name__ == '__main__':
    print("æµ‹è¯•ï¼šç¿»è¯‘æˆ–è€…æ€»ç»“\n")
    op = input("è¾“å…¥è¦è°ƒç”¨çš„API: ")
    if op == 'ç¿»è¯‘':
        text = input("è¾“å…¥è¦ç¿»è¯‘çš„å†…å®¹ï¼š")
        lang = input("è¾“å…¥è¦ç¿»è¯‘çš„è¯­è¨€ï¼š")
        translate_test(text, lang)
    elif op == "æ€»ç»“":
        text = input("è¾“å…¥è¦æ€»ç»“çš„é•¿æ–‡æœ¬ï¼š")
        word_limit = int(input("è¾“å…¥é™åˆ¶çš„å­—æ•°ï¼š"))
        summary_test(text, word_limit)
```

#### **å‰ç«¯æ¸²æŸ“**

**é¦–å…ˆæ„å»º API å®¢æˆ·ç«¯ï¼Œå°è£…è¯·æ±‚**

```python
import requests

BASE_URL = "http://127.0.0.1:8028"

def api_translate(text: str, target_lang: str):
    """è°ƒç”¨åç«¯ç¿»è¯‘æ¥å£"""
    url = f"{BASE_URL}/api/translate"
    payload = {
        "text": text,
        "target_lang": target_lang
    }

    print("æ­£åœ¨è°ƒç”¨ç¿»è¯‘ API...")

    try:
        response = requests.post(url, json=payload)
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"API è°ƒç”¨å¤±è´¥: {response.text}"}
    except requests.exceptions.ConnectionError:
        return {"error": "æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡ï¼Œè¯·æ£€æŸ¥ FastAPI æ˜¯å¦å·²å¯åŠ¨ (Port 8028)"}


def api_summary(text: str, word_limit: int):
    """è°ƒç”¨åç«¯æ‘˜è¦æ¥å£"""
    url = f"{BASE_URL}/api/summary"
    payload = {
        "text": text,
        "word_limit": word_limit
    }
    print("æ­£åœ¨è°ƒç”¨æ€»ç»“æ‘˜è¦ API...")
    try:
        response = requests.post(url, json=payload)
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"API è°ƒç”¨å¤±è´¥: {response.text}"}
    except requests.exceptions.ConnectionError:
        return {"error": "æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡ï¼Œè¯·æ£€æŸ¥ FastAPI æ˜¯å¦å·²å¯åŠ¨ (Port 8028)"}
```

**æ„å»ºå‰ç«¯**

```python
import streamlit as st
import sys
import os
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç»å¯¹è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
# å°†è¿™ä¸ªè·¯å¾„åŠ å…¥åˆ° Python çš„æœç´¢è·¯å¾„ (sys.path) ä¸­
if current_dir not in sys.path:
    sys.path.append(current_dir)
# åˆ«ç†ä¼š IDE çš„æŠ¥é”™
from utils.client_script import api_summary, api_translate

st.set_page_config(page_title="Atri tools box", page_icon="ğŸ§°", layout="wide")
st.title("ğŸš€Atri tools box")

st.sidebar.title("åŠŸèƒ½å¯¼èˆª")
page = st.sidebar.radio("é€‰æ‹©å·¥å…·", ["ğŸŒ æ™ºèƒ½ç¿»è¯‘", "ğŸ“ æ–‡ç« æ‘˜è¦"])

if page == "ğŸŒ æ™ºèƒ½ç¿»è¯‘":
    st.header("å¤šè¯­è¨€æ™ºèƒ½ç¿»è¯‘")

    # å·¦å³å¸ƒå±€ï¼šå·¦è¾¹è¾“å…¥ï¼Œå³è¾¹æ˜¾ç¤ºç»“æœ
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("è¾“å…¥")
        input_text = st.text_area("è¯·è¾“å…¥è¦ç¿»è¯‘çš„æ–‡æœ¬", height=200, placeholder="åœ¨æ­¤è¾“å…¥...")
        target_lang = st.selectbox(
            "é€‰æ‹©ç›®æ ‡è¯­è¨€",
            ["English", "Chinese", "Japanese", "French", "German", "Spanish"]
        )

        submit_btn = st.button("å¼€å§‹ç¿»è¯‘", use_container_width=True)

    with col2:
        st.subheader("ç»“æœ")
        result_container = st.empty()

        if submit_btn and input_text:
            with st.spinner("AI æ­£åœ¨æ€è€ƒä¸­..."):
                result = api_translate(input_text, target_lang)

                if "error" in result:
                    st.error(result["error"])
                else:
                    st.success("ç¿»è¯‘å®Œæˆï¼")
                    st.text_area("è¯‘æ–‡", value=result['translated_text'], height=150)
                    st.info(f"æ£€æµ‹åˆ°çš„æºè¯­è¨€: {result['detected_language']}")
                    
elif page == "ğŸ“ æ–‡ç« æ‘˜è¦":
    st.header("é•¿æ–‡æœ¬æ™ºèƒ½æ‘˜è¦")
    input_text = st.text_area("è¯·è¾“å…¥é•¿æ–‡ç« ", height=250)
    # æ»‘å—æ§åˆ¶å­—æ•°
    word_limit = st.slider("æ‘˜è¦å­—æ•°é™åˆ¶", min_value=50, max_value=500, value=100, step=10)
    if st.button("ç”Ÿæˆæ‘˜è¦"):
        if not input_text:
            st.warning("è¯·å…ˆè¾“å…¥æ–‡æœ¬ï¼")
        else:
            with st.spinner("AI æ­£åœ¨é˜…è¯»æ–‡ç« å¹¶æ€»ç»“..."):
                result = api_summary(input_text, word_limit)

                if "error" in result:
                    st.error(result["error"])
                else:
                    st.divider()
                    st.subheader("ğŸ“„ æ‘˜è¦å†…å®¹")
                    st.write(result['summary'])

                    st.subheader("ğŸ·ï¸ å…³é”®æ ‡ç­¾")
                    try:
                        st.pills("Tags", result['tags'])
                    except AttributeError:
                        st.write(" | ".join([f"`{tag}`" for tag in result['tags']]))
# streamlit run llmcalling/project_two/fronted/fronted.py
```

#### **è¿è¡Œæ•ˆæœ**

åç«¯ fastAPI æ‰§è¡Œ `python main.py`ï¼Œå‰ç«¯ Streamlit æ‰§è¡Œ `streamlit run llmcalling/project_two/fronted/fronted.py`

![1](./1.jpg)

Summary [Issue #8](https://github.com/JuyaoHuang/AI-agent/issues/8)ï¼š

![2](./2.jpg)

```bash
Project 2's core MVC architecture is complete, and the next phase involves extending functionality through three main tasks: implementing an /api/summary endpoint using PromptFactory and a dedicated LLM flow, creating a Python client simulation script to test API behavior, and integrating the Streamlit frontend from Project 1 with the new FastAPI backend. The plan includes refactoring the LLM service for reusability, using the requests library for client-side API calls, and rendering responses in Streamlitâ€”potentially with streaming output for better UX.
```

---

## LLM é‡æ„

å°†å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨çš„éƒ¨åˆ†æŠ½è±¡ä¸ºä¸€ä¸ªæ–°çš„æ–¹æ³•ï¼Œæä¾›æ¥å£ï¼Œæ¥æ”¶è¿”å›çš„æ–‡æœ¬å—ã€‚

**æ³¨æ„**ï¼šä¸€èˆ¬çš„ web è¯·æ±‚æµç¨‹éƒ½æ˜¯ï¼šå‰ç«¯å‘é€è¯·æ±‚ -> åç«¯å‘é€ llm è¯·æ±‚ -> æ¥æ”¶å®Œæ•´å†…å®¹/æµå¼å— -> åç«¯å°†å“åº”å†…å®¹å‘ç»™å‰ç«¯æ¸²æŸ“ã€‚å› æ­¤åç»­å‰åç«¯é›†æˆæ—¶éœ€è¦ä½¿ç”¨æµå¼è¾“å‡ºï¼Œå¢å¼ºç”¨æˆ·çš„ä½“éªŒã€‚è¿™æ ·å°±æ— æ³•ä½¿ç”¨è¿”å›å®Œæ•´çš„å“åº”åå†å°†å…¶ä¼ ç»™å‰ç«¯äº†ï¼Œè€Œæ˜¯**å°†è°ƒç”¨ llm æ—¶è¿”å›çš„æ¯ä¸€ä¸ªå—éƒ½å°†å…¶ä¼ ç»™å‰ç«¯**

ç”±æ­¤ï¼ŒLLM è°ƒç”¨æ—¶åº”æä¾›ä¸‰ä¸ªæ–¹æ³•ï¼Œåˆ†åˆ«å¯¹åº”äºï¼š

1. **å•æ¬¡å®Œæ•´è¾“å‡º**ï¼šåç«¯æ„å»ºè¯·æ±‚å®Œæ•´çš„å“åº”æ–‡æœ¬ content
2. **æµå¼è¾“å‡º**ï¼šä½¿ç”¨æµå¼è¾“å‡ºçš„æ–¹å¼è¿”å›å“åº”æ–‡æœ¬ï¼Œä¸ä½¿ç”¨æ€è€ƒæ¨¡å¼ï¼Œå› æ­¤ä¸éœ€è¦è¿›è¡Œ**ç»“æ„åŒ–**æµå¼ä¼ è¾“ï¼Œåªéœ€è¦ä¼ è¾“æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬å†…å®¹å³å¯ã€‚
3. **æ€è€ƒæ¨¡å¼**ï¼šç”±äºä½¿ç”¨æµå¼è¾“å‡ºæ—¶ä¸èƒ½è®©å¤§æ¨¡å‹è¾“å‡ºçš„å†…å®¹ä¸º json æ ¼å¼ï¼Œå› æ­¤éœ€è¦åœ¨æ¥æ”¶å¤§æ¨¡å‹çš„è¾“å‡ºå**æ‰‹åŠ¨æ„å»ºç»“æ„åŒ–çš„æµå¼è¾“å‡º**ã€‚

> æ³¨æ„ï¼š**å¯ç”¨æµå¼è¾“å‡ºæ—¶ï¼Œæ¨¡å‹ä¸åº”è¯¥ä½¿ç”¨ json æ ¼å¼è¾“å‡ºï¼Œè€Œæ—¶çº¯æ–‡æœ¬æ ·å¼**ã€‚

**åä¸¤ç§å®ç°æ¯”è¾ƒå¤æ‚ï¼Œæœ¬å®è·µå…ˆä½¿ç”¨ç¬¬ä¸€ç§æ–¹å¼ï¼Œåç»­å†åšå•ç‹¬çš„æ‰©å±•å’Œä»‹ç»**ã€‚

DeepSeek çš„ APP åº”ç”¨é‡Œä½¿ç”¨çš„å°±æ˜¯ç¬¬ä¸‰ç§ï¼Œæ€è€ƒæ¨¡å¼ã€‚

### å®ç°æ€è·¯

**1. å•æ¬¡å®Œæ•´è¾“å‡º**

è¿™ä¸ªæ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œå°±æ˜¯ç›´æ¥ `return response.choices[0].message.content`ï¼Œå³è¿”å›æ¨¡å‹çš„è¾“å‡ºå³å¯ã€‚

**2. æµå¼è¾“å‡º**

è¦å®ç°æµå¼è¾“å‡ºï¼Œå°±è¦æ”¹å˜ä¸¤ä¸ªæ ¸å¿ƒé€»è¾‘ï¼š

1. LLM è°ƒç”¨å±‚ï¼šä¸èƒ½ä¸€æ¬¡æ€§ `return` ç»“æœï¼Œè€Œæ˜¯**ä½¿ç”¨ Python çš„ç”Ÿæˆå™¨ `yield`**ï¼ŒåƒæŒ¤ç‰™è†ä¸€æ ·æŠŠæ•°æ®ä¸€ç‚¹ç‚¹æŒ¤å‡ºæ¥ã€‚
2. fastAPI æœåŠ¡å±‚ï¼šä¸èƒ½è¿”å›æ™®é€šçš„ JSON å¯¹è±¡ï¼Œè€Œæ˜¯è¦è¿”å› StreamingResponseï¼Œå®ƒä¼šå»ºç«‹ä¸€ä¸ªé•¿è¿æ¥ï¼ŒæŠŠ LLM æŒ¤å‡ºæ¥çš„æ•°æ®å®æ—¶æ¨ç»™å‰ç«¯
   

**æ³¨æ„**ï¼š
1. **æµå¼è¾“å‡º**å’Œ**è¦æ±‚æ¨¡å‹è¾“å‡ºæ ¼å¼ä¸º JSON**è¿™ä¸¤ç‚¹æ˜¯è¿èƒŒçš„ã€‚å› ä¸ºå¯ç”¨æµå¼è¾“å‡ºæ—¶ï¼Œå¾ˆéš¾å°† JSON æ ¼å¼çš„æ•°æ®æå–å“åº”æ–‡æœ¬ï¼ˆå› ä¸ºæ˜¯å—çŠ¶æ•°æ®ï¼Œä¸æ˜¯ä¸€æ¡å®Œæ•´çš„å“åº”æ–‡æœ¬ï¼‰
2. **æµå¼è¾“å‡ºä¸‹**ï¼Œæµå¼æ¥å£ä¸€èˆ¬ä½¿ç”¨ SSE (Server-Sent Events) åè®®å®šä¹‰æ¨¡å‹æˆ–è€…**ä¸éœ€è¦ä½¿ç”¨ Pydantic æ¨¡å‹**ä¼ è¾“å“åº”

**å®ç°ä»£ç **ï¼š

1. åœ¨è°ƒç”¨å¤§æ¨¡å‹æ—¶ï¼Œæ„å»ºæ–°çš„æ–¹æ³•ï¼Œè€Œä¸”å°†å…¶å˜ä¸ºç”Ÿæˆå™¨ã€‚

   ```python
   def llmcalling_stream_block(model: str, messages: list, temperature: int) -> Generator[str, None, None]:
       yield content
   ```

2. ä¿®æ”¹ fastAPI æœåŠ¡å±‚ï¼Œä½¿ç”¨æµå¼å“åº” StreamingResponse ä¼ è¾“æ–‡æœ¬

   ```python
   return StreamingResponse(stream_generator, media_type="text/event-stream")
   ```

3. å¯ä½¿ç”¨è„šæœ¬æ¨¡æ‹Ÿå‰ç«¯çš„è¯·æ±‚å’Œæ˜¾ç¤ºæƒ…å†µ

**3. æ€è€ƒæ¨¡å¼**

æ€è€ƒæ¨¡å¼æ¯”è¾ƒå¤æ‚ï¼Œå› ä¸ºæ¨¡å‹è¾“å‡ºæ˜¯çº¯ markdown æ ¼å¼ï¼Œéœ€è¦è‡ªå·±**æ‰‹åŠ¨æ„å»º JSON æ ¼å¼è¿›è¡Œä¼ è¾“**ã€‚æ¢è¨€ä¹‹ï¼Œéœ€è¦å‰åç«¯çº¦å®šæ•°æ®æ ¼å¼ï¼Œå®ç°**ç»“æ„åŒ–æµå¼ä¼ è¾“**ã€‚

æˆ‘ä»¬éœ€è¦æŒ‡å®šä¸€ä¸ªç®€å•çš„åè®®çº¦å®šï¼Œå‘é€ JSON å­—ç¬¦ä¸²ï¼Œæ¯ä¸€è¡Œå¸¦ä¸Šç±»å‹æ ‡ç­¾ï¼Œå‘Šè¯‰å‰ç«¯è¿™ä¸€å—æ•°æ®æ˜¯æ€è€ƒå†…å®¹è¿˜æ˜¯å›å¤å†…å®¹ã€‚

ä¾‹å¦‚ï¼š
```bash
{"type": "thinking", "content": "æˆ‘æ­£åœ¨åˆ†æ..."}
{"type": "thinking", "content": "æ£€ç´¢æ•°æ®åº“..."}
{"type": "answer", "content": "ä½ å¥½"}
{"type": "answer", "content": "ï¼Œ"}
{"type": "answer", "content": "ä¸–ç•Œ"}
```

**å…³é”®ä»£ç å®ç°**ï¼š

1. LLM è°ƒç”¨æ¨¡å—ï¼š
   
    æ–°å»º llmcalling_thinking æ–¹æ³•ï¼Œè®©å®ƒè¿”å›å¸¦æœ‰ç±»å‹æ ‡ç­¾çš„ JSON å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯çº¯æ–‡æœ¬ã€‚
    
    ```python
    @staticmethod
    def llmcalling_thinking(model: str, messages: list, temperature: int) -> str:
      for chunk in response:
      # 1. å¤„ç† token ç»Ÿè®¡ä¿¡æ¯
      if not chunk.choices:
          if chunk.usage:
              yield json.dumps({
                  "type": "usage",
                  "content": chunk.usage.model_dump()# è½¬æˆå­—å…¸
              }, ensure_ascii=False) + "\n"
    
      delta = chunk.choices[0].delta
    
      # 2. å¤„ç†æ€è€ƒè¿‡ç¨‹
      reasoning = getattr(delta, "reasoning_content", None)
    
      if reasoning:
          data = {
              "type": "thinking",
              "content": reasoning
          }
          yield json.dumps(data) + "\n"
    
      # 3. å¤„ç† Content
      elif delta.content:
          data = {
              "type": "answer",
              "content": delta.content
          }
          yield json.dumps(data, ensure_ascii=False) + "\n"
    ```

2. fastAPI æœåŠ¡å±‚
   
   ä½¿ç”¨ NDJSON åè®®è¿›è¡Œ json å—çš„æ•°æ®ä¼ è¾“
   ```python
    # è¿”å›æµå¼å“åº”
    # media_type="application/x-ndjson" è¡¨ç¤º "Newline Delimited JSON"
    # ä¸€ç§æ ‡å‡†çš„æŒ‰è¡Œåˆ†å‰² JSON çš„æ ¼å¼
    return StreamingResponse(stream_generator, media_type="application/x-ndjson")
   ```
   
   > å¦‚æœä½ ç”¨æ™®é€šçš„ application/jsonï¼Œæµè§ˆå™¨ä¼šæœŸå¾…æ”¶åˆ°ä¸€ä¸ªåˆæ³•çš„ JSON å¯¹è±¡ï¼ˆæ¯”å¦‚ä¸€ä¸ªå·¨å¤§çš„ {...} æˆ– [...]ï¼‰ã€‚åœ¨æ•°æ®æµä¼ è¾“å®Œæˆä¹‹å‰ï¼Œæµè§ˆå™¨é€šå¸¸è®¤ä¸º JSON è¿˜æ²¡æ¥æ”¶å®Œï¼Œå¯èƒ½ä¸ä¼šè§¦å‘æ¸²æŸ“ï¼Œæˆ–è€…æŠ¥é”™ã€‚
   >
   > è€Œ NDJSON (Newline Delimited JSON) æ˜¯ä¸€ç§æ ‡å‡†åè®®ï¼Œå®ƒçš„è§„åˆ™æ˜¯ï¼š
   >
   > - æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ã€åˆæ³•çš„ JSON å¯¹è±¡
   > - è¡Œä¸è¡Œä¹‹é—´ç”¨ \n åˆ†éš”
   >
   > å‰ç«¯æ”¶åˆ°è¿™ç§ Header åçš„ååº”ï¼š
   >
   > å‰ç«¯ï¼ˆæ¯”å¦‚ç”¨ fetch APIï¼‰çœ‹åˆ°è¿™ä¸ª content-typeï¼Œæˆ–è€…åœ¨å¤„ç†æµæ—¶ï¼Œå°±çŸ¥é“åº”è¯¥**è¯»ä¸€è¡Œï¼Œè§£æä¸€è¡Œï¼Œæ¸²æŸ“ä¸€è¡Œ**ï¼Œè€Œä¸æ˜¯å‚»å‚»åœ°ç­‰æ•´ä¸ªè¯·æ±‚ç»“æŸ
   

**æ€»ç»“**

- **æ„å»º**ï¼šç”¨ json.dumps(...) + "\n"
- **ä¼ è¾“**ï¼š ç”¨ StreamingResponse
- **å£°æ˜**ï¼š ç”¨ media_type="application/x-ndjson"

è¿™å¥—ç»„åˆæ‹³æ˜¯ç›®å‰å¤„ç† LLM å¤æ‚æµå¼è¾“å‡ºï¼ˆå¦‚ DeepSeek/OpenAI çš„ function callingï¼‰çš„è¡Œä¸šæ ‡å‡†åšæ³•ã€‚

---

## å®è·µæ‰©å±•äºŒ

### è¦æ±‚

> ç›¸å…³Issueï¼šhttps://github.com/JuyaoHuang/AI-agent/issues/11

ä¸ºäº†æå‡ç”¨æˆ·ä½“éªŒï¼Œæˆ‘ä»¬éœ€è¦ä»å½“å‰çš„â€œç­‰å¾…å®Œå…¨ç”Ÿæˆåä¸€æ¬¡æ€§è¿”å›â€æ¨¡å¼ï¼Œå‡çº§ä¸ºâ€œæµå¼è¾“å‡ºâ€æ¨¡å¼ã€‚ç‰¹åˆ«é’ˆå¯¹æ”¯æŒæ·±åº¦æ€è€ƒçš„æ¨¡å‹ï¼ˆå¦‚ `qwen3-vl-32b-thinking` / DeepSeek-R1ï¼‰ï¼Œåç«¯éœ€è¦èƒ½å¤ŸåŒºåˆ†å¹¶å®æ—¶è¿”å› **æ€è€ƒè¿‡ç¨‹** å’Œ **æ­£å¼å›å¤ **ï¼Œä»¥ä¾¿å‰ç«¯èƒ½å¤Ÿå°†å®ƒä»¬åˆ†å¼€æ¸²æŸ“ï¼ˆä¾‹å¦‚ï¼šæ€è€ƒè¿‡ç¨‹æ˜¾ç¤ºåœ¨æŠ˜å é¢æ¿ä¸­ï¼‰

**ğŸ¯ å®ç°ç›®æ ‡**

- LLM è°ƒç”¨å±‚æ–°å¢è°ƒç”¨æ–¹æ³•
  - å°† `llmcalling` æ–¹æ³•æ‰©å±•æ”¯æŒ `stream=True`ã€‚
  - é€‚é… `extra_body` å‚æ•°ä»¥å¼€å¯æ€è€ƒæ¨¡å¼ï¼ˆ`enable_thinking: True`ï¼‰
  - å®ç°ç”Ÿæˆå™¨ (Generator)ï¼Œèƒ½å¤ŸåŒºåˆ† `reasoning_content` å’Œ `content`
  - å¤„ç† `usage` ä¿¡æ¯ï¼ˆToken æ¶ˆè€—ç»Ÿè®¡ï¼‰
- å®šä¹‰æµå¼ä¼ è¾“åè®®
  - é‡‡ç”¨ **NDJSON (Newline Delimited JSON)** æ ¼å¼ä¼ è¾“æ•°æ®
  - å®šä¹‰æ¶ˆæ¯ç±»å‹ï¼š`thinking` (æ€è€ƒä¸­), `answer` (å›ç­”ä¸­), `usage` (ç»Ÿè®¡)
- æ›´æ–° FastAPI æœåŠ¡å±‚
  - æ–°å¢æ¥å£ `/api/translate/stream-thinking`
  - ä½¿ç”¨ `StreamingResponse` è¿”å›æµå¼æ•°æ®
  - ç¡®ä¿ `media_type` è®¾ç½®ä¸º `application/x-ndjson`



### 1. å•æ¬¡è¯·æ±‚å®Œæ•´çš„å“åº”

å®ç°æœ€ç®€å•çš„ã€æœ€ç¨³å®šçš„ï¼Œåœ¨ API è¯·æ±‚é‡Œæœ€å¸¸ä½¿ç”¨çš„æ–¹å¼ï¼šä¸€æ¬¡è¯·æ±‚ç›´æ¥è¿”å›å®Œæ•´çš„æ–‡æœ¬å›å¤ï¼ˆJSONæ ¼å¼è¿”å›ï¼‰ã€‚

#### **prompt æ„å»º**

```python
f"""
ä½ æ˜¯ä¸€ä¸ªç²¾é€šå¤šå›½è¯­è¨€çš„èµ„æ·±ç¿»è¯‘å¼•æ“ã€‚
ä»»åŠ¡ï¼š
1. å°†ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ç¿»è¯‘æˆ {target_lang}ã€‚
2. è‡ªåŠ¨æ£€æµ‹åŸæ–‡çš„è¯­è¨€ã€‚
3. å¿…é¡»ä¸¥æ ¼ä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
   - translated_text: ç¿»è¯‘åçš„å†…å®¹
   - source_lang: åŸæ–‡çš„è¯­è¨€ï¼ˆå¦‚ Chinese, English, Frenchï¼‰
ç¤ºä¾‹ï¼š
{{
    "translated_text": "hello",
    "source_lang": "en"
}}
"""
```

#### **LLM è°ƒç”¨æ„å»º**

```python
class LLMCalling:
    @staticmethod
    def llmcalling(model: str, messages: list, temperature: float) -> str:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
        )

        return response.choices[0].message.content
```

#### **æœåŠ¡å±‚ API è¯·æ±‚æ–¹æ³•**

```python
@app.post("/api/translate", response_model=TranslateResponse)
async def translate(request: TranslateRequest):
    try:
        system_prompt = PromptFactory.get_translate_prompt(request.target_lang)

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": request.text},
        ]
        content = LLMCalling.llmcalling("qwen3-max", messages, 0.3)

        data = json.loads(content)

        return TranslateResponse(
            original_text=request.text,
            translated_text=data.get("translated_text", "Fail to translate"),
            detected_language=data.get("source_lang", "unknown"),
        )
    except Exception as e:
        print(f"Error:{e}\n")
        raise HTTPException(status_code=502, detail=str(e))
```

#### **æµ‹è¯•è„šæœ¬**

```python
import requests

url = "http://127.0.0.1:8026/api/translate"

payload = {
    "text": "Life is like a box of chocolates.",
    "target_lang": "Chinese"
}

print("æ­£åœ¨å‘¼å«åç«¯...")
response = requests.post(url, json=payload)

if response.status_code == 200:
    data = response.json()
    print(f"ç¿»è¯‘ç»“æœ: {data['translated_text']}")
    print(f"è¯†åˆ«è¯­è¨€: {data['detected_language']}")
else:
    print("è°ƒç”¨å¤±è´¥:", response.text)
```

è¾“å‡ºï¼š
```bash
æ­£åœ¨å‘¼å«åç«¯...
ç¿»è¯‘ç»“æœ: ç”Ÿæ´»å°±åƒä¸€ç›’å·§å…‹åŠ›ã€‚
è¯†åˆ«è¯­è¨€: English
```











