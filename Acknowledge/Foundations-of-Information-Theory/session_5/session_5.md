---
title: 无失真信源编码
published: 2026-01-06
description: "信息量基础：无失真信源编码"
first_level_category: "知识库"
second_level_category: "信息论"
tags: ['信息论']
draft: false
---

## 补充：熵率的介绍

熵率是用来衡量一个信源产生信息的平均速率，即平均每个符号携带多少信息量。

离散随机变量 $X$ 的熵 $H(X)$ 是针对单个符号的，因此对于无记忆信源，熵率 $H_{\infty}$ 就等于单个符号的熵 $H(X)$。

但在现实中的信源（如一段文字、一段语音）通常输出的是一个序列 $X_1, X_2, \dots, X_n$。

- 如果是无记忆信源，每个符号之间互不相关，那么看单个 $H(X)$ 就够了。
- 如果是有记忆信源（比如英文文章，“q”后面大概率跟着“u”），**符号之间存在相关性**。这种相关性会使得序列整体的不确定性降低。此时就需要用熵率来描述随着序列变长，平均到每个符号上的 新信息量 是多少。

### 数学定义

设信源为一个随机过程 $\mathcal{X} = \{X_1, X_2, \dots, X_n, \dots\}$。

信源的熵率通常记为 $H(\mathcal{X})$ 或 $H_{\infty}$，定义为 $n$ 个符号的联合熵除以 $n$，当 $n$ 趋于无穷大时的极限：
$$
H_{\infty} = \lim_{n \to \infty} \frac{1}{n} H(X_1, X_2, \dots, X_n) 
$$

其中$H(X_1, X_2, \dots, X_n)$ 是序列 $X^n$ 的联合熵。这个公式的物理意义是：当信源序列足够长时，平均每个符号所贡献的熵（不确定性）。

### 示例情况

**1. 离散无记忆信源 (DMS)**

如果信源是无记忆的，即 $X_1, X_2, \dots$ 相互独立且同分布，那么联合熵等于各分量熵之和：
$$
H(X_1, X_2, \dots, X_n) = \sum_{i=1}^{n} H(X_i) = n H(X) 
$$
代入熵率公式：
$$
H_{\infty} = \lim_{n \to \infty} \frac{1}{n} [n H(X)] = H(X) 
$$
即对于无记忆信源，熵率就等于单个符号的熵。

**2. 平稳有记忆信源**
如果信源是有记忆的（符号间有依赖关系），则：
$$
H(X_1, X_2, \dots, X_n) < \sum_{i=1}^{n} H(X_i) 
$$
这意味着联合熵的增长速度慢于 $n H(X)$。此时熵率 $H_{\infty}$ 会小于单个符号的熵 $H(X)$。

对于平稳信源，熵率还有另一个等价定义（利用条件熵）：
$$
H_{\infty} = \lim_{n \to \infty}H_N(X) = \lim_{n \to \infty} H(X_n | X_{n-1}, X_{n-2}, \dots, X_1) 
$$
其意义是：已知前面所有历史符号的情况下，当前这个符号带来的“新”信息量。

*平稳信源的熵率等于最小的平均符号熵*

### 核心定义

熵率有其他角度的定义：**极限平均符号熵，以及极限条件熵**。 三者描述的都是同一个东西：一个信源产生信息的平均速率，即平均每个符号携带多少信息量

**1. 熵率是平均符号熵的极限**

这是熵率最直观的定义。如果把长度为 $N$ 的序列 $(X_1, X_2, \dots, X_N)$ 的联合熵记为 $H(X_1, X_2, \dots, X_N)$，那么 "平均每个符号的熵" 就是除以 $N$。

当 $N$ 趋于无穷大时，这个平均值的极限就是熵率 $H_\infty$：

$$
H_\infty = \lim_{N \to \infty} \frac{1}{N} H(X_1, X_2, \dots, X_N) 
$$

**2. 熵率是条件熵的极限**

对于有记忆信源的计算，熵率也可用极限条件熵来描述。

它表示：在已知前面所有 $N-1$ 个符号的历史记录的情况下，第 $N$ 个符号带来的不确定性（即新信息量）。当历史记录无限长时，这个条件熵的极限也是熵率：

$$
H_\infty = \lim_{N \to \infty} H(X_N | X_1, X_2, \dots, X_{N-1}) 
$$

-----

而对于平稳信源（即统计特性不随时间推移而改变的信源），这两个定义是等价的：

$$
\lim_{N \to \infty} \frac{1}{N} H(X^N) = \lim_{N \to \infty} H(X_N | X^{N-1}) 
$$

## 考点（核心内容）

### 1. 编码方法与类型
*   二元 Huffman 编码 (Binary Huffman Coding)
*   定长编码 (Fixed-Length Coding)
*   变长编码 (Variable-Length Coding)
*   异前置码 / 前缀码 (Prefix Code / Instantaneous Code)
*   唯一可译码 (Uniquely Decodable Code)
*   最优编码 (Optimal Coding)

### 2. 性能指标与计算
*   信源熵 (Source Entropy)
*   平均码长 (Average Code Length)
*   编码效率 (Coding Efficiency)
*   存储空间 / 信息量 (Storage Space / Information Content)
*   错误概率 (Error Probability) —— *针对定长码截断*

### 3. 定理与判定条件
*   香农第一定理 / 无失真信源编码定理 (Shannon's First Theorem / Lossless Source Coding Theorem)
*   Kraft 不等式 (Kraft Inequality) —— *异前置码存在的充要条件*
*   码字所需最小长度 (Minimum Code Length) —— *针对定长码*

### 4. 信源特性与进阶概念
*   无记忆信源 (Memoryless Source)
*   信源扩展 (Source Extension) —— *逼近熵下界的方法*
*   联合信源 / 联合熵 (Joint Source / Joint Entropy) —— *利用相关性消除冗余*
*   信源相关性 (Source Correlation)
*   典型序列 (Typical Sequences) —— *隐含在5.2题的子集编码中*

## 1. 概述

### 1.1. 原信源的N次扩展码

![6](./6.jpg)

### 1.2. 信源编码分类

![6](./7.jpg)

### 1.3. 分组码（码集、码字、码符号）定义

![6](./8.jpg)

由题，码符号有两个：0、1。因此码符号数 r 为 2。

*码集合可以简写为 “码”*
### 1.4. 分组码特性

**1. 非奇异码**

非奇异码就是一个码（集）中各码字不同，否则称为奇异码。

**2. 唯一可译码**

唯一可译码就是对于不同的消息序列，对应着不同的码序列。换句话说，对于不同信源产生的信源符号，对应编码的码字不同。

例如：

信源符号集合 = {a, b, c, d}，对应的编码码字为 = {00 , 01, 11, 10}。
- 码中各个码字不同，因此为非奇异码
- 符号集合里每一个符号都由一个不同的码字编码，因此为唯一可译码

![6](./9.jpg)


**3. 即时码**

![10](./10.jpg)

#### 1.5. 异前置码

![6](./11.jpg)


**人话**：

**1. 前缀**：

对于一个码字序列：$X = \{ x_1x_2x_3...x_n\}$，由其前 $K$ 个码字 $X_K = \{ x_1x_2x_3...x_k\}$ 构成的序列，称 $X$ 的前缀。

**2. 异前置码**：

如果码（集合）中的任何一个码字都不是其他码字的前缀，那么这个码（集合）称为异前置码。

**所以异前置码一定是唯一可译码，且等价为即时码**。

![12](./12.jpg)

## 2. 定长码

### 2.1. 定长码无失真编码条件

其中 $l$ 为码长。

![12](./13.jpg)

![12](./14.jpg)

*定长码不考，跳过*

## 3. 变长编码

### 3.1. Kraft 不等式

![15](./15.jpg)

![16](./16.jpg)


设码字长度集合为 $\{l_1, l_2, \dots, l_q\}$，则必须满足：
$$
\sum_{i=1}^{q} r^{-l_i} \le 1 
$$
其中 $r$ 是进制数（二元编码时 $r=2$）。

**满足 Kraft 不等式等价于 存在异前置码**

**1. 该不等式的人话**：

对一个信源符号数为 $q$ 的序列进行编码，编码使用的符号数总数为 $r$，然后每一个信源符号编码后的码长为 $l_i$。

**2. 例一：**

1. 码符号为 $\{0 , 1\} $，因此 $r = 2$。
2. 信源符号数有 5 个，每一个符号的概率如图。

![17](./17.jpg)

| 信源符号 | 白色  | 黑色  | 红色  | 黄色  | 蓝色  |
| :------: | :---: | :---: | :---: | :---: | :---: |
| **码长** | **2** | **2** | **2** | **3** | **3** |

所以：

- $q = 5$ 
- $r = 2$
- $L = \{2, 2, 2, 3, 3\}$

由公式：
$$
\sum_{i=1}^{q} r^{-l_i} \le 1
$$
计算得到结果：

![18](./18.jpg)

**3. 重要性**

![18](./19.jpg)

### 3.2. 变长信源编码定理

*重点是第一点平均码长的计算*

![18](./20.jpg)
![18](./21.jpg)

**1. 平均码长人话**：

衡量编码后平均每个信源符号需要多少个二进制位来表示的指标。

$$ \bar{L} = \sum_{i=1}^{q} p(x_i) l_i \quad (\text{单位：bit/symbol}) $$

* $p(x_i)$：第 $i$ 个符号出现的概率。
* $l_i$：第 $i$ 个符号对应的码字长度（例如 '01' 的长度为 2）。

**例子**：

续例一，根据题目的**每个信源符号的概率和码长**可以计算得到平均码长：

![22](./22.jpg)

**2. 单符号信源变长编码定理**

![23](./23.jpg)

**3. 有限序列**

![23](./24.jpg)

**4. 平稳马氏源**

![23](./25.jpg)

![23](./26.jpg)

### 3.3. 性能指标总结

**其中 $\bar{l}$ 为平均码长**：
$$
\bar{l} = \sum_{i=1}^{q} p(x_i) l_i \quad (\text{单位：bit/symbol}) 
$$
*   $p(x_i)$：第 $i$ 个符号出现的概率
*   $l_i$：第 $i$ 个符号对应的码字长度（例如 '01' 的长度为 2）

![23](./27.jpg)

*后文有配题的总的性能指标小结。*

**例二**

![23](./28.jpg)
![23](./29.jpg)

### 3.4. 无失真信源编码定理 ---- 香农第一定理

**1. 介绍**

香农第一定理，又称为无失真信源编码定理，是信息论中关于数据压缩最基础且最重要的理论界限。

香农第一定理确立了信源信息的**压缩极限**。该定理指出，信源的信息熵（熵率）是无失真编码后平均码长的下界。任何试图将平均码长压缩至低于该下界的编码方案，都无法实现无失真恢复，必然会导致信息的丢失。

**2. 数学表达式**

设 $X$ 为离散无记忆信源，其信息熵为 $H(X)$。若对该信源进行编码，且平均码长（或编码速率）为 $\bar{L}$，则无失真编码必须满足：
$$
\bar{L} \ge H(X) 
$$

*平均码长 $\bar{L}$ 和编码速率 $R$ 是同一个意思。*

且通过对信源进行 $N$ 次扩展（将 $N$ 个符号作为一个整体进行编码），当 $N$ 趋于无穷大时，平均每个符号的码长可以无限逼近熵值：
$$ \lim_{N \to \infty} \frac{\bar{L}_N}{N} = H(X) $$

**3. 性质**

- 下界存在（$R \ge H$）：

    信息熵 $H$ 代表了信源内在的、本质的不确定性或信息量。无论采用何种编码方法（如 Huffman 编码、算术编码等），编码后的平均比特数 $R$ 都不可能少于信源本身含有的平均信息量 $H$。
- 界限可达：
  
    虽然单符号编码可能无法精确达到熵值（除非概率分布是 2 的负幂次），但定理保证了只要对足够长的信源序列进行编码（利用大数定律和渐近均分特性），就存在一种编码方法，使其平均码率任意接近 $H$。
- 不可逾越（不存在 $R < H$ 的无失真编码）：
  
    如果强行要求平均码长 $R$ 小于熵 $H$，则信源输出的某些信息必然无法被唯一表示。这就像试图将体积为 $H$ 的水装入容积为 $R$ 的杯子中，若 $R < H$，水必然会溢出（即发生信息丢失/失真）。

**4. 实际计算**

1. 计算理论下界信源熵 $H(X)$
   
   $$
   H(X) = - \sum_{i=1}^{q} p(x_i) \log_2 p(x_i) 
   $$
2. 计算实际指标平均码长 $\bar{L}$
   
   $$
   \bar{L} = \sum_{i=1}^{q} p(x_i) l_i 
   $$
3. 将计算出的 $\bar{L}$ 与 $H(X)$ 进行比较：
   
   - $\bar{L} = H(X)$：该编码达到了无失真编码定理的下界，即编码效率  $\eta = 100\%$。
   - $\bar{L} > H(X)$：该编码未达到下界（但在该编码规则下可能已经是最好的了，只是受限于信源分布）

-----

![30](./30.jpg)

![30](./31.jpg)

## 4.Huffman 编码

*不讲，网上讲的很多。*

## 5. 性能指标计算总结

### 1. 信源熵
$$
 H(X) = - \sum_{i=1}^{q} p(x_i) \log_2 p(x_i) \quad (\text{单位：bit/symbol}) 
$$

*   $q$：信源符号的个数（例如题目中的5种颜色，则 $q=5$）。
*   $p(x_i)$：第 $i$ 个符号出现的概率。

### 2. 平均码长
衡量编码后平均每个信源符号需要多少个二进制位来表示的指标。

$$
\bar{L} = \sum_{i=1}^{q} p(x_i) l_i \quad (\text{单位：bit/symbol})
$$

*   $p(x_i)$：第 $i$ 个符号出现的概率。
*   $l_i$：第 $i$ 个符号对应的码字长度（例如 '01' 的长度为 2）

### 3. 编码效率
衡量实际编码的平均码长接近理论极限（熵）的程度。

$$
\eta = \frac{H(X)}{\bar{L}} \times 100\%
$$

*   由于香农第一定理 $\bar{L} \ge H(X)$，因此 $0 < \eta \le 1$。
*   $\eta$ 越接近 $100\%$，编码越优。

### 4. 编码冗余度
衡量编码中浪费的空间比例。

$$
\gamma = 1 - \eta = 1 - \frac{H(X)}{\bar{L}}
$$

### 5. 总存储空间 / 总信息量
计算一段长度为 $N$ 的信源序列编码后占用的总比特数（例如题目中的 “二维码存储空间” 或 “观众记录空间” ）。

$$
S_{total} = N \times \bar{L} \quad (\text{单位：bits})
$$

*   $N$：信源序列的总长度（例如 100 个像素点，10000 名观众）。
*   $\bar{L}$：上面计算出的平均码长。

## 综合示例

![32](./32.jpg)

![32](./33.jpg)
![32](./34.jpg)
