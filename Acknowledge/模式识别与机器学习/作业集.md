---
title: '作业集'
publishDate: 2025-12-28
description: "模式识别与机器学习作业集"
tags: ['ML']
language: 'Chinese'
first_level_category: "知识库"
second_level_category: "机器学习"
draft: false
---

## 作业一

![1](./imgs/1.png)

### Q1

机器（计算机）通过记忆和计算解决了使用知识的问题，实现了自动化；而机器学习的核心在于解决了知识自动获取的问题，从而实现了智能化。模式识别提供了具体的问题场景，机器学习提供了获取知识的方法，二者结合构成了人工智能的核心基础。

### Q2：ML 的三大支柱

- **定义**：研究如何让计算机系统通过数据获得改善智能模型（函数）性能的知识和规律，并运用模型进行预测和决策。
- **三大支柱**：计算机、数据、**学习方法（核心）**

### Q3：ML 的推理方式

机器学习采用的是基于数据的归纳推理，也被称为实例学习。

其推理逻辑主要分为两步：

- 归纳（学习过程）：通过学习算法从大量具体的“数据”中总结出普遍规律，构建出智能函数模型 $p=f_\theta(x)$。
- 演绎（预测过程）：利用归纳出的模型对新的输入进行处理，从而得到“预测”结果。

就是从具体数据中归纳出模型，再用模型对未知情况进行推断。

## 作业二

![10](./imgs/10.jpg)

### Q1

1. **为何基于统计学？**

   机器学习旨在从**有限样本**中归纳出适用于**未知数据**的规律。统计学为此提供了处理数据随机性、从样本推断总体、以及量化泛化误差（如过拟合分析）的数学基石。

2. **不确定性及其来源**

   不确定性指无法对事件做确切描述或预测。主要来源包括：

   - **固有随机性**：事物本身或环境的物理噪声。
   - **观测不全**：关键信息缺失或传感器精度限制。
   - **认知局限**：模型假设过于简化或训练数据不足。

3. **是否都有不确定性？**

   绝大多数现实 AI 问题都包含不确定性；仅少数完全信息的封闭逻辑问题（如简单数学证明）例外。

4. **大模型是否必须处理？**

   **必须。** 处理不确定性是解决大模型“幻觉”、评估回答可信度的关键。模型只有具备“知道自己不知道”的能力，才能在医疗、驾驶等高风险领域安全应用。

### Q2：机器学习三要素

- 模型形式
- 准则函数
- 求解算法

### Q3：理解机器学习中的模型

1.  **函数表示**
    智能问题的本质是寻找一个从输入特征 $\boldsymbol{x}$ 到输出目标 $y$ 的最佳映射函数 $y=f(\boldsymbol{x}; \boldsymbol{\theta})$。输入 $\boldsymbol{x}$ 是对客观事物的数字化描述，输出 $y$ 是决策结果。

2.  **模型种类**
    主要依据函数形式分为：
    *   线性模型：函数是输入的线性组合（如线性判别、逻辑回归），计算简单但拟合能力有限。
    *   非线性模型：能处理复杂关系（如多层感知机、带核函数的SVM）。
    *   此外还可按建模方式分为生成模型（贝叶斯）与判别模型。

3.  **参数作用**
    参数（如权重 $\boldsymbol{w}$、偏置 $b$）决定了函数的具体形态（如决策边界的位置、曲线的弯曲程度）。机器学习的核心就是通过算法不断调整参数值，使模型对数据的预测误差最小化，从而“学”到规律。

### Q4：准则函数是什么

1.  **准则函数的作用**

    准则函数（又称损失函数或目标函数）是衡量模型预测性能优劣的数学标准，也是模型学习的驱动力。它将学习问题转化为数学上的优化问题：通过量化模型预测值与真实值之间的误差，指导算法调整参数，使误差最小化。

2.  **如何构建更好的准则函数**
    构建的核心在于平衡拟合能力与泛化能力：

    *   理论基础：我们的最终目标是最小化期望风险，但由于无法获取全部数据，实际上无法直接计算。
    *   实际操作：通常使用经验风险作为替代。
    *   改进方法：为了防止仅最小化经验风险导致的过拟合，通常采用结构风险最小化策略。即在经验风险函数中加入正则化项（惩罚模型复杂度），以此约束模型，使其在未知数据上也能表现良好。

## 作业三

![11](./imgs/11.jpg)

### Q1：泛化能力

**1. 什么是泛化能力**

泛化能力是指模型对**未见过的、新的数据**（测试集）进行准确预测的能力。它是机器学习的核心目标，即模型不仅要记住训练数据，更要能学到普遍规律以适应未知样本。

**2. 影响泛化能力的主要因素：**

**模型复杂度**：

* 模型过于简单会出现**欠拟合**；

* 模型过于复杂会出现**过拟合**，即模型死记硬背了训练数据中的噪声。

*   泛化误差上界公式表明：
    $$
     R(f) \le \hat{R}(f) + \sqrt{\frac{h(\ln \frac{2N}{h} + 1) - \ln \frac{\eta}{4}}{N}} 
    $$
    其中 $h$ 为模型复杂度。**$h$ 越大，右侧的置信范围越宽，泛化能力越差。**

**训练样本数 ($N$)**：

样本数是泛化能力的保证。观察上式，分母中包含样本数 $N$。当 $N$ 增大时，第二项（惩罚项）减小，经验风险 $\hat{R}(f)$ 更接近真实的期望风险 $R(f)$，从而提高泛化能力。

**准则函数**：

仅最小化经验风险（即训练误差）容易导致过拟合。

为了提升泛化能力，准则函数应遵循**结构风险最小化**原则，加入正则化项：
$$
J(\theta) = \sum_{i=1}^{N} L(y_i, f(x_i)) + \lambda \Omega(\theta) 
$$
通过 $\lambda \Omega(\theta)$ 惩罚模型复杂度，在拟合精度和模型简单性之间取得平衡。

### Q2：求解优化问题的算法

**1. 优化算法的任务形式**

优化算法的核心任务是在参数空间中寻找一组最优参数 $\boldsymbol{\theta}^*$，使得模型定义的准则函数（目标函数） $J(\boldsymbol{\theta})$ 达到极值）

数学表达为：
$$
\boldsymbol{\theta}^* = \arg \min_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) 
$$
即通过算法迭代，使模型的预测误差最小化。

**2. 常见的优化算法**
* **梯度下降法**：最常用的迭代算法。利用一阶导数信息，沿梯度的反方向更新参数。更新公式为：
  $$
  \boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \eta \nabla J(\boldsymbol{\theta}^{(t)})
  $$
  其中 $\eta$ 为学习率。常见的变体有 SGD 和 Adam 等。

* **牛顿法**：利用海森矩阵信息，收敛速度通常快于梯度下降，但计算复杂度高。

* **拉格朗日乘子法**：专门用于解决带约束条件的优化问题（如支持向量机 SVM 的推导）。

*   **解析法**：对于线性回归等简单模型，可直接求导令为 0 得到闭式解。

### Q3：最优决策方法

**1. 决策方法**

在已知所有分类知识（即先验概率 $P(\omega_i)$ 和类条件概率密度 $p(\mathbf{x}|\omega_i)$）的情况下，应采用**贝叶斯决策**。
核心是计算**后验概率**，即在观测到特征 $\mathbf{x}$ 后，样本属于某类 $\omega_i$ 的概率：
$$
P(\omega_i|\mathbf{x}) = \frac{p(\mathbf{x}|\omega_i)P(\omega_i)}{\sum_{j} p(\mathbf{x}|\omega_j)P(\omega_j)} 
$$
通常采用**最大后验概率准则**：若 $P(\omega_k|\mathbf{x}) > P(\omega_j|\mathbf{x})$ ($\forall j \neq k$)，则决策为 $\omega_k$

**2. 是否最优**

**是**，这是理论上的最优决策。贝叶斯决策充分利用了已知的概率分布信息。

数学上可以证明，**最小错误率贝叶斯决策**能使总体的平均错误率达到理论下界（贝叶斯误差）；若考虑不同错误的代价（损失），则**最小风险贝叶斯决策**能使期望风险最小。

## 作业四

![12](./imgs/12.jpg)

### Q1：不确定性的表示

在存在不确定性的预测任务中，我们通过**概率分布**来定量表示不确定性，并利用**贝叶斯推断**进行演算：

1. **表示**：首先确立先验概率 $P(\omega)$，代表对预测状态（如“下雨”）的初始置信度；

2. **观测**：收集当前环境的数据 $\mathbf{x}$（如湿度、云层），并计算**类条件概率** $p(\mathbf{x}|\omega)$，量化数据与状态的匹配程度；

3. **演算**：利用**贝叶斯公式**融合先验知识与观测证据，计算**后验概率**：
   $$
    P(\omega|\mathbf{x}) = \frac{p(\mathbf{x}|\omega)P(\omega)}{\sum_{i} p(\mathbf{x}|\omega_i)P(\omega_i)} 
   $$
   后验概率即为结合了当前信息后修正的预测结果，从而实现了在不确定环境下的最优推断。

### Q2：如何得到最优决策

在不确定性环境下，获取最优决策的关键在于根据任务需求选择合适的准则，并利用贝叶斯决策理论进行推导：

1. **场景一：所有错误代价相等**

   若仅需关注准确率，应采用**最小错误率贝叶斯决策**。通过计算各类的后验概率 $P(\omega_i|\mathbf{x})$，直接选择概率最大的类别，从而保证总体错误率最低

2. **场景二：错误代价不同**

   若不同错误的后果严重程度不同（如漏报火灾），必须采用**最小风险贝叶斯决策**。

   首先定义损失函数 $\lambda(\alpha_i, \omega_j)$，计算采取某种行动 $\alpha_i$ 的条件风险：
   $$
   R(\alpha_i|\mathbf{x}) = \sum_{j} \lambda(\alpha_i, \omega_j) P(\omega_j|\mathbf{x}) 
   $$
   最终选择使条件风险 $R$ 最小的决策行动，以此达到期望损失最小的最优解。

### Q3：ML 与统计决策的关系

机器学习与统计决策理论是实现手段与理论基石的关系。

1.  统计决策理论为机器学习提供了数学框架和评估标准。它定义了在不确定性下什么是最优决策，并给出了模型性能的理论上限。
2.  虽然早期 AI 包含符号逻辑等非统计方法，但现代机器学习**核心建立在统计学之上**。机器学习算法本质上是从有限数据中估计未知的概率分布（如 $P(\omega|\mathbf{x})$），试图逼近统计决策理论所定义的理想最优解。

## 作业五

![13](./imgs/13.jpg)

### Q1：最小错误率决策的例子

**最小错误率决策**是指以使分类错误的平均概率最小为目标的贝叶斯决策方法。其核心规则是计算样本属于各类的后验概率，并将其归类于概率最大的一类。

假设需区分生产线上的零件是合格品 ($\omega_1$) 还是次品 ($\omega_2$)。

观测到某零件的尺寸特征 $\mathbf{x}$，结合历史数据（先验）和分布规律（似然），经贝叶斯公式算出后验概率为：
$$
P(\omega_1|\mathbf{x}) = 0.8, \quad P(\omega_2|\mathbf{x}) = 0.2
$$
根据**最小错误率准则**，因 $0.8 > 0.2$，系统判定该零件为**合格品**。此时决策犯错（即实际上它是次品）的概率仅为 $0.2$；若反之判定为次品，犯错概率则高达 $0.8$。选择后验概率最大的一类，即保证了每次决策的出错概率最小。

### Q2：最小风险决策的例子

**最小风险决策**是指在决策中引入损失函数，综合考虑后验概率与不同错误带来的代价，选择使期望损失（条件风险）最小的行动。

假设进行癌症筛查，模型计算出某患者患癌概率 $P(\text{癌})=0.2$，健康概率 $P(\text{正常})=0.8$。若仅看概率（最小错误率），应判为“健康”。

但在现实中，两类错误的代价悬殊：
*   漏诊（将癌症判为健康）：可能延误治疗导致死亡，设损失值 $\lambda=100$。
*   误诊（将健康判为癌症）：仅增加复查费用和心理负担，设损失值 $\lambda=10$。

此时计算决策风险：
*   判为“健康”的风险：$0.2 \times 100 + 0.8 \times 0 = 20$。
*   判为“癌症”的风险：$0.2 \times 0 + 0.8 \times 10 = 8$。

因为 $8 < 20$，尽管患癌概率较低，为了规避巨大风险，最优决策仍是判为**“疑似癌症”**并建议复查。

### Q3

当各类的**协方差矩阵相等**时（即 $\boldsymbol{\Sigma}_i = \boldsymbol{\Sigma}_j = \boldsymbol{\Sigma}$），分类边界是线性函数。

**原因**：

高斯分布的对数判别函数 $g_i(\mathbf{x})$ 通常包含关于特征向量 $\mathbf{x}$ 的二次项 $-\frac{1}{2}\mathbf{x}^T \boldsymbol{\Sigma}_i^{-1} \mathbf{x}$。

当协方差矩阵相等时，该二次项对所有类别都是相同的。在求解决策边界方程 $g_i(\mathbf{x}) - g_j(\mathbf{x}) = 0$ 时，二次项相互抵消，只剩下 $\mathbf{x}$ 的一次项和常数项。
此时线性判别函数形式为：
$$
 g_i(\mathbf{x}) = \boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_i \cdot \mathbf{x} + C_i 
$$
故决策边界为线性超平面。

## 作业六

![55](./imgs/55.jpg)

### Q1：参数估计与非参估计

**课本 P44**

**1. 参数估计**

假设概率密度函数具有**已知的数学形式**（如高斯分布），仅有部分参数未知。
问题的核心是将密度估计转化为对这些未知参数（如均值 $\boldsymbol{\mu}$ 和协方差矩阵 $\boldsymbol{\Sigma}$）的计算。

**常用方法**：极大似然估计、贝叶斯参数估计。

**2. 非参数估计**

**不假设**概率密度函数的先验形式，完全依赖训练数据本身来确定概率分布的形状。
它通常利用样本在特征空间中的局部分布密度来近似整体概率密度，适用于数据分布复杂或未知的情况。

**常用方法**：Parzen 窗法、$k$ 近邻法。

### Q2

**1. 相似性与认同**

**是的，认同**。这个过程模拟了人类的**归纳学习**机制——即从具体的过往经验（样本）中总结出普遍规律（模型），并将其应用于未知场景（预测）。在机器学习中，“学习”的本质就是利用数据来优化模型性能，参数估计正是通过数据修正信念（参数）以逼近客观真理的过程。

**2. 理想的统计机器学习**

我认为理想的统计机器学习不仅仅是数据的拟合，而应具备强大的**泛化能力**和**可解释性**。它应该能从有限样本中捕捉数据背后的**本质结构**，遵循奥卡姆剃刀原则，用最简洁的模型解释复杂现象，并对分布外的未知数据具有鲁棒性。

### Q3：参数估计的主要方法

**最大似然估计**

- **核心观点**：把待估参数看作是**固定但未知**的常数。
- **原理**：寻找一组参数值，使得观测到当前样本集出现的概率（即似然函数）最大。

**贝叶斯估计**

- **核心观点**：把待估参数看作是**随机变量**，服从某种已知的先验分布。
- **原理**：利用样本信息修正对参数的先验认识，计算参数的**后验概率密度**来进行估计。

两者的主要区别在于是否将参数视为随机变量。

## 作业七

![56](./imgs/56.jpg)

**已知条件：**

- 极大似然估计： 视参数为未知定值，仅已知样本和概率密度形式。
- 贝叶斯估计 & 贝叶斯学习： 视参数为随机变量，除样本外，还已知参数的先验分布。

**估计目标：**

- 极大似然估计： 找到一个使样本出现概率最大的参数点估计值。
- 贝叶斯估计： 得出参数的后验概率密度分布（而非一个数值）。
- 贝叶斯学习： 得出新样本 $x$ 的概率密度函数 $P(x|D)$（即总体分布）。

**具体方法：**

- 极大似然估计： 构造似然函数，通过求导等方式求最大值。
- 贝叶斯估计： 利用贝叶斯公式，用样本修正先验，计算参数的后验分布。
- 贝叶斯学习： 利用全概率公式，对参数的后验分布进行积分（加权平均）。

## 作业八

![58](./imgs/57.jpg)

### Q1：线性回归模型

**1. 解决的问题**

线性回归模型解决的是**回归**问题，即预测连续的实数值输出，而非分类问题（尽管它也可用于分类的基础）。

**2. 模型形式**

模型假设输出是输入特征的线性组合。对于输入向量 $\mathbf{x}$，模型形式为：
$$
f(\mathbf{x}) = \mathbf{w}^T\mathbf{x} + b
$$
其中 $\mathbf{w}$ 是权重向量，$b$ 是偏置。若使用增广向量形式，则简化为 $f(\mathbf{x}) = \mathbf{w}^T\mathbf{x}$。

**3. 准则函数**

通常使用平方误差损失（最小二乘准则）来衡量预测值与真实值 $y$ 之间的差异：
$$
J(\mathbf{w}) = \sum_{i=1}^{N} (y_i - \mathbf{w}^T\mathbf{x}_i)^2 = ||\mathbf{y} - \mathbf{X}\mathbf{w}||^2
$$
**4. 求解方法**

主要有两种求解策略：

- 最小二乘法：通过求导令梯度为零得到解析解（闭式解）：
  $$
  \hat{\mathbf{w}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
  $$

- **梯度下降法**：通过迭代更新权重来最小化损失函数，适用于数据量过大无法求逆矩阵的情况。

### Q2：逻辑回归模型

**与生成模型的联系**

逻辑回归虽是判别模型，但其后验概率形式可由生成模型推导得出。当假设两类数据的类条件概率密度 $p(\mathbf{x}|C_k)$ 服从**协方差矩阵相同的高斯分布**时，根据贝叶斯公式计算出的后验概率 $P(C_1|\mathbf{x})$ 恰好具有 Logistic Sigmoid 函数的形式。

**模型的形式**
使用Sigmoid函数将线性组合映射到 $(0,1)$ 区间来表示概率：
$$
P(y=1|\mathbf{x}) = \sigma(\mathbf{w}^T\mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T\mathbf{x}}}
$$
**准则函数**

通常使用**交叉熵损失函数**（即最小化负对数似然）：
$$
E(\mathbf{w}) = -\sum_{n=1}^N \{t_n \ln y_n + (1-t_n) \ln (1-y_n)\}
$$
**求解方法**

由于是非线性模型，不存在闭式解（解析解）。通常使用**梯度下降法**或**牛顿-拉夫森法**（IRLS）进行迭代求解。

### Q3：Fisher 线性判别函数

**模型的形式**

Fisher 线性判别将高维输入向量 $\mathbf{x}$ 投影到一维空间（直线） $y$ 上：
$$
y = \mathbf{w}^T\mathbf{x}
$$
其核心不在于直接回归或分类，而是寻找最佳投影方向 $\mathbf{w}$，随后通过设定阈值进行分类。

**准则函数**

采用 **Fisher 准则**，即最大化投影后的**类间散度与类内散度之比**。旨在让两类样本的均值距离尽可能远，同时各自内部尽可能紧凑：
$$
J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}
$$
其中 $\mathbf{S}_B$ 是类间离散度矩阵，$\mathbf{S}_W$ 是总类内离散度矩阵。

**求解方法**

对 $J(\mathbf{w})$ 求导并令其为 0，利用拉格朗日乘子法求解广义特征值问题。对于二分类问题，最佳投影方向有闭式解：
$$
\mathbf{w}^* = \mathbf{S}_W^{-1}(\mathbf{m}_1 - \mathbf{m}_2)
$$
其中 $\mathbf{m}_1, \mathbf{m}_2$ 为两类样本的原始均值向量。

### Q3：感知器

**模型形式**
感知器是一个二分类的线性模型，通过符号函数将输入特征的线性组合映射为类别标签（通常取 $\{-1, +1\}$）：
$$
f(\mathbf{x}) = \text{sign}(\mathbf{w}^T\mathbf{x})
$$
（此处假定偏置 $b$ 已并入权重向量 $\mathbf{w}$）。

**目标函数**

采用**感知器准则函数**。它仅针对**误分类样本**计算代价，定义为所有误分类样本到决策超平面的距离之和（忽略常数系数）。设 $\mathcal{M}$ 为误分类样本集合：
$$
J(\mathbf{w}) = - \sum_{\mathbf{x}_i \in \mathcal{M}} y_i (\mathbf{w}^T\mathbf{x}_i)
$$
当所有样本正确分类时，$J(\mathbf{w})=0$。

**求解方法**

采用**随机梯度下降法**。对于某个被误分类的样本 $(\mathbf{x}_i, y_i)$，沿梯度的反方向更新权重，直到收敛：
$$
\mathbf{w} \leftarrow \mathbf{w} + \eta y_i \mathbf{x}_i
$$
其中 $\eta$ 为学习率。

## 作业九

![59](./imgs/58.jpg)

### Q1：逻辑回归模型

**与生成模型的联系**
逻辑回归属于判别模型，但其后验概率形式与特定的生成模型密切相关。当假设两类数据的类条件概率密度 $p(\mathbf{x}|C_k)$ 服从**协方差矩阵相同的高斯分布**时，通过贝叶斯公式推导出的后验概率 $P(C_1|\mathbf{x})$ 恰好具有 Logistic Sigmoid 函数的形式。

**模型的形式**
采用 Sigmoid 函数将输入特征的线性组合映射为概率值（$(0,1)$ 区间）：
$$
y = \sigma(\mathbf{w}^T\mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T\mathbf{x}}}
$$
**准则函数**
使用**交叉熵损失函数**，其本质是最大化数据的对数似然函数。对于二分类问题，目标是最小化：
$$
E(\mathbf{w}) = -\sum_{n=1}^N \{t_n \ln y_n + (1-t_n) \ln (1-y_n)\}
$$
**求解方法**
由于 Sigmoid 引入了非线性，该准则函数没有解析解。通常采用**梯度下降法**或**牛顿-拉夫森法**进行迭代优化求解。

### Q2：前馈神经网络

**书上P100页更好**

**总之，就是多层单向**。

![62](./imgs/61.jpg)

![59](./imgs/59.png)

### Q3：简述误差反向传播法

误差反向传播算法（BP算法）是训练多层前馈神经网络的核心方法，其本质是利用**链式法则**计算损失函数对权重的梯度，并通过**梯度下降**来最小化误差。

算法主要分为两个阶段：
1.  **前向传播**：输入信号经过各隐层的加权和激活函数处理传至输出层，计算实际输出与期望值的误差。
2.  **反向传播**：将误差梯度从输出层向输入层逐层反向传递，计算每个神经元的误差项 $\delta$。
    
    对于隐层单元 $j$，其误差项由后一层单元 $k$ 的误差项加权回传：
    $$
    \delta_j = f'(net_j) \sum_{k} w_{kj} \delta_k
    $$
3.  **权重更新**：根据计算出的梯度调整权重：
    $$
    w_{ji} \leftarrow w_{ji} + \Delta w_{ji}, \quad \Delta w_{ji} = -\eta \frac{\partial E}{\partial w_{ji}} = \eta \delta_j x_i
    $$
    其中 $\eta$ 为学习率。

![60](./imgs/60.jpg)

## 作业十

![60](./imgs/62.jpg)

### Q2

人工神经网络（ANN）本质上是受生物神经元启发的**通用函数拟合器**。

**从机器学习角度**：ANN 突破了传统线性模型的限制。通过深度学习和非线性激活函数，它具备极强的**表征学习能力**，能够自动从原始数据中提取高层抽象特征，避免了繁琐的人工特征工程。理论上，它可以以任意精度逼近任何连续函数。

**从智能对比角度**：尽管 ANN 在模式识别方面表现卓越，甚至超越人类，但它主要依赖**大数据的统计相关性**，缺乏人类具备的因果推理、常识逻辑以及“举一反三”的小样本学习能力。它目前更像是“强大的计算工具”而非真正的“智能体”。

### Q3

> 异或是一种逻辑运算，仅当两个输入不同时输出1，相同时输出0。在几何上，其四个样本点（(0,0), (1,1)为0类；(0,1), (1,0)为1类）无法被一条直线分开，属于典型的**线性不可分**问题，必须使用多层神经网络。

**实现步骤**

1.  **语言选择**：首选 Python。
2.  **寻找 Baseline**：在 GitHub 搜索关键词 "XOR neural network python" 或 "MLP implementation from scratch" 获取开源参考代码。
3.  **模型构建**：搭建一个MLP，核心是至少包含一个隐藏层（通常2个神经元以上）并使用非线性激活函数（ ReLU）。
4.  **结果显示**：使用 **Matplotlib** 库打印预测准确率和绘制**决策边界 **，直观展示网络如何形成非线性曲线来划分数据。

## 作业十一

![60](./imgs/63.jpg)

参数模型（如逻辑回归）假设固定的函数形式，参数数量固定且有限；非参数模型（如 kNN）不预设形式，参数数量随样本量增加而增长，实质上是“样本即参数”。

参数模型依赖**特征或基函数**拟合全局规律；非参数模型依赖**相似度函数**（如距离度量）进行局部推断。

参数模型训练完成后丢弃数据，仅存参数；非参数模型需**存储所有训练样本**，测试时实时调用样本计算。

非参数模型因必须记住全部样本而得名。现代大模型（LLM）虽属参数模型，但通过海量参数**隐式记忆**了大量训练细节，或通过检索增强（RAG）结合外部存储，融合了非参数模型的记忆特性以弥补幻觉。

---

**模型参数**

参数模型（如线性回归）假设数据符合特定分布，参数数量固定且独立于样本量（如权重向量 $\mathbf{w}$）。非参数模型（如 $k$-NN）不对数据分布做强假设，没有固定的参数集，其“参数”本质上是整个训练数据集，且模型规模随样本量 $N$ 的增加而增长。

**特征或基函数 vs 相似度函数**

参数模型通常通过一组固定的基函数（或特征）的线性组合来构建决策面，关注全局拟合。非参数模型则依赖**相似度函数**（如欧氏距离）来度量新样本与存储样本的接近程度，利用局部邻域信息进行推断。

**训练样本在训练时和测试时的作用**
参数模型在训练阶段利用样本估计参数，训练完成后样本即可丢弃，测试时仅需参数。非参数模型属于“懒惰学习”，训练阶段通常仅存储数据，而将计算推迟到测试阶段，测试时必须保留并遍历所有训练样本。

**为什么非参数模型叫做“记忆模型”**

因为它没有将数据压缩为抽象的规则或参数，而是显式地**存储（记忆）** 了所有曾经见过的训练实例。预测过程类似于在大脑记忆中检索相似的历史案例，并依据这些案例的标签直接做出判断。

**思考大模型中记忆模型的特点**

现代大模型通常结合了两者：**参数记忆**（权重）存储通用语言模式和逻辑，具有泛化能力但更新困难；**非参数记忆**（如 RAG 检索增强）通过外挂知识库存储事实性信息，具有精准、可即时更新和可回溯来源的特点，弥补了纯参数模型的幻觉和滞后问题。

## 作业十二

![60](./imgs/64.jpg)

### Q1：KNN 与 SVM 的关系

**1. 参数模型与非参数模型**

KNN 是典型的**非参数模型**，需存储所有训练样本；SVM 是**参数模型**（或稀疏核机），通过优化求解得到固定参数。联系：SVM 可被视为一种“稀疏化”的近邻方法，它不需要存储所有数据，只保留了对决策边界有贡献的少数样本（支持向量）。

**2. 边界样本与支持向量**

KNN 的分类结果由测试点周围的局部样本（往往位于类边界附近）决定；SVM 的决策超平面完全由支持向量决定，这些支持向量正是位于两类边界边缘的最难分样本。两者都体现了“分类边界由少数关键的边缘样本决定”这一思想。

**3. 距离算法与核函数**

KNN 使用距离（如欧氏距离）度量差异；SVM 使用核函数度量相似度。两者在数学上是联系的，例如常用的高斯 RBF 核函数单调依赖于欧氏距离：
$$
K(\mathbf{x}, \mathbf{z}) = \exp(-\gamma \|\mathbf{x} - \mathbf{z}\|^2)
$$
这使得使用 RBF 核的 SVM 在形式上等价于一种基于支持向量的加权近邻分类器。

### Q2：SVM 介绍

**1. 最大间隔分类器**
SVM 旨在寻找一个决策超平面，使得两类样本中离超平面最近的点（支持向量）到平面的距离最大。基于统计学习理论，最大化间隔能最小化结构风险，从而提升模型的**泛化能力**。目标函数转化为最小化权重范数：$\min \frac{1}{2}\|\mathbf{w}\|^2$

**2. 对偶表示与核函数**

通过拉格朗日对偶性，原问题转化为对偶问题，决策函数仅依赖于样本间的内积。这允许引入**核技巧**，用核函数 $K(\mathbf{x}, \mathbf{z})$ 替代内积，在不显式计算高维映射的情况下处理非线性分类。

**3. 稀疏模型**

SVM 是**稀疏模型**。根据 KKT 条件，最终模型参数中，只有少数位于间隔边界上的样本（支持向量）对应的拉格朗日乘子 $a_n > 0$，其余绝大多数样本的 $a_n = 0$。预测公式仅由这些支持向量决定：
$$
y(\mathbf{x}) = \sum_{n \in SV} a_n t_n K(\mathbf{x}_n, \mathbf{x}) + b
$$
**4. 求解方法**

该问题本质是凸二次规划（QP）问题，通常采用**SMO（序列最小优化）算法**进行高效求解。

## 作业十三

![60](./imgs/65.jpg)

### Q1：监督学习与无监督学习

**监督学习**：主要解决**分类**（离散输出）和**回归**（连续输出）问题。其范式是基于包含“输入-标签”对的训练集 $\{(\mathbf{x}_n, t_n)\}$，学习一个映射函数 $y(\mathbf{x})$，从而对新样本进行预测。目标是最小化预测值与真实标签之间的误差。

**无监督学习**：主要解决**聚类**、**降维**和**密度估计**问题。其范式是仅提供输入数据 $\{\mathbf{x}_n\}$ 而无对应标签，旨在挖掘数据内在的聚类结构、相关性或概率分布规律。

**区别**：核心差异在于是否有标签 $t$。监督学习有明确的“标准答案”指导优化，而无监督学习是自发探索数据的隐含结构。

### Q2： KNN 聚类算法

**问题的定义**

K均值聚类是一种无监督学习方法，旨在将 $N$ 个无标签样本划分为 $K$ 个互不相交的簇，使得簇内样本尽可能紧密，而簇间样本尽可能远离。

**模型的形式**

模型由 $K$ 个原型向量（聚类中心）$\{\boldsymbol{\mu}_k\}$ 表示。引入二值指示变量 $r_{nk} \in \{0, 1\}$，若数据点 $\mathbf{x}_n$ 属于第 $k$ 簇，则 $r_{nk}=1$，否则为 $0$。

**目标函数**

最小化所有样本点到其所属聚类中心的距离平方和：
$$
J = \sum_{n=1}^N \sum_{k=1}^K r_{nk} \|\mathbf{x}_n - \boldsymbol{\mu}_k\|^2
$$
**求解方法**

采用迭代优化策略：

1.  **分配步**：固定 $\boldsymbol{\mu}_k$，将每个样本 $\mathbf{x}_n$ 分配给距离最近的中心（确定 $r_{nk}$）。
2.  **更新步**：固定 $r_{nk}$，重新计算每个簇的中心 $\boldsymbol{\mu}_k$ 为该簇所有样本的均值。
    重复上述步骤直至收敛。

![60](./imgs/66.jpg)

### Q1：特征空间优化与分类器

**任务的形式**
特征空间优化旨在改变数据的表示形式，通过降维或特征选择去除冗余和噪声（$\mathbf{x} \to \mathbf{z}$）；分类器设计则是在固定特征空间下建立输入到类别的映射规则（$\mathbf{x} \to y$）

**模型**
特征优化的模型通常是变换矩阵或特征子集掩码；分类器设计的模型是判别函数或概率分布模型（如 $y = f(\mathbf{w}^T\mathbf{x})$）

**目标函数**
特征优化关注数据的**内在性质**（如信息量、可分性），例如 PCA 最大化方差：
$$
J(\mathbf{w}) = \mathbf{w}^T \mathbf{S} \mathbf{w}
$$
分类器设计关注**预测结果的准确性**，致力于最小化损失函数（如均方误差或交叉熵）：
$$
E(\mathbf{w}) = \sum_{n} L(t_n, y_n)
$$
**优化算法**

特征优化常涉及**特征值分解**（广义特征值问题）或组合搜索；分类器设计多采用**梯度下降法**或二次规划求解。

### Q2：无监督特征优化

主要常见的无监督特征优化（特征提取）方法包括主成分分析 (PCA) 和独立成分分析 (ICA)。

**1. 主成分分析 (PCA)**

* 模型：线性投影模型 $\mathbf{y} = \mathbf{W}^T\mathbf{x}$，将高维数据映射到低维正交子空间，旨在去除相关性并保留主要信息。

*   目标函数：最大化投影后的样本方差（或最小化重构均方误差）。
    
    在正交约束 $\mathbf{W}^T\mathbf{W}=\mathbf{I}$ 下：
    $$
    J(\mathbf{W}) = \text{tr}(\mathbf{W}^T \mathbf{S} \mathbf{W})
    $$
    其中 $\mathbf{S}$ 为样本协方差矩阵。
    
*   优化算法：特征值分解。计算 $\mathbf{S}$ 的特征值，选取前 $d$ 个最大特征值对应的特征向量构成变换矩阵。

**2. 独立成分分析 (ICA)**

*   模型：生成模型 $\mathbf{x} = \mathbf{A}\mathbf{s}$，假设观测数据由统计独立的非高斯源信号混合而成。
*   目标函数：最大化估计信号的非高斯性（通常使用峰度或负熵度量），或最大化似然函数。
*   优化算法：梯度上升法或不动点迭代算法。

### Q3：深度学习与无监督优化学习

**联系**

两者都致力于特征提取。深度学习可以看作是一种自动化的特征工程，其隐层通过多层非线性变换将原始数据映射到特征空间。

*特别是自编码器，其不仅是无监督学习，且目标也是最小化重构误差，与 PCA 思想高度一致。*

**区别**

**任务与目标**：

*   **无监督特征优化**（如 PCA）：关注数据的通用统计特性（如方差、独立性），目标函数通常独立于最终任务（例如最小化重构误差 $J = \|\mathbf{x} - \hat{\mathbf{x}}\|^2$）。
*   **深度学习**（监督）：特征优化由最终任务（如分类）驱动，通过最小化预测损失（如交叉熵 $E = -\sum t \ln y$）来调整特征，提取的是最具判别性的特征。

**模型与算法**：
*   前者多为**线性模型**（如矩阵变换），常通过特征值分解求解解析解。
*   后者是**深层非线性模型**，需利用反向传播和梯度下降法进行迭代求解。
